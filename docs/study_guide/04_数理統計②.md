# AIに必要な数理・統計知識②

> **G検定 学習ガイド** | 分野：数理統計② | 37P | 2026-02-25

---

## 確率の基礎

確率とは、ある事象が起こる可能性の度合いを示すものです。AIや機械学習では、データの傾向を理解したり、将来を予測したりするために確率の考え方が不可欠です。

### 確率変数

**確率変数**とは、試行の結果によって値が決まる変数のことです。取りうる値が分かっていて、それぞれの値をとる確率が与えられています。

例えば、サイコロを1回振るという試行を考えます。
*   サイコロの出る目は1, 2, 3, 4, 5, 6のいずれかです。
*   それぞれの目が出る確率は1/6です。

この場合、サイコロの目（X）は確率変数であると言えます。

### 確率分布

**確率分布**とは、確率変数が取りうる値と、その値をとる確率を対応させて示したものです。これにより、確率変数の振る舞いを全体的に把握できます。

サイコロの例で確率分布を表にすると以下のようになります。

| 確率変数 X (サイコロの目) | 1   | 2   | 3   | 4   | 5   | 6   |
| :------------------------ | :-- | :-- | :-- | :-- | :-- | :-- |
| 確率 P(X)                 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |

> 📌 **G検定頻出ポイント**:
> 確率変数は「試行の結果で値が決まる変数」、確率分布は「確率変数の取りうる値と確率の関係」を指します。この2つの概念はAIの分野で頻繁に登場するため、しっかりと理解しておきましょう。

### 期待値

**期待値**とは、確率変数が取りうる値とその値をとる確率の積の和のことです。簡単に言えば、1回の試行で得られる平均的な値を示します。

サイコロの例で期待値を計算してみましょう。
期待値 E(X) = (1 × 1/6) + (2 × 1/6) + (3 × 1/6) + (4 × 1/6) + (5 × 1/6) + (6 × 1/6)
E(X) = (1 + 2 + 3 + 4 + 5 + 6) / 6 = 21 / 6 = 3.5

サイコロを何度も振ると、平均して3.5の目が出ることが期待されます。

### 条件付き確率

**条件付き確率**とは、「ある事象Aが起こるという条件下で、別の事象Bが起こる確率」のことです。P(B|A)と表記され、「Aが与えられたときのBの確率」と読みます。

**計算式**: P(B|A) = P(AかつB) / P(A)

**例**: ある試験の受験生のうち、50%が合格者であり、女性でかつ合格者は30%であるとします。合格者の中からランダムに1人選んだとき、その人が女性である確率を求めます。

*   事象A：試験の合格者 (P(A) = 0.5)
*   事象B：女性
*   事象AかつB：女性でかつ合格者 (P(AかつB) = 0.3)

この場合、合格者の中から選んだ人が女性である確率（P(B|A)）は、
P(B|A) = P(AかつB) / P(A) = 0.3 / 0.5 = 0.6

したがって、合格者の中から選んだ人が女性である確率は60%となります。

> 📌 **G検定頻出ポイント**:
> 条件付き確率は、ベイズの定理の基礎となる重要な概念です。AIの分野では、ある情報（条件）が与えられたときに、別の事象が起こる確率を推定する際に頻繁に利用されます。

---

## 確率密度

これまでの確率は、サイコロの目やコインの裏表のように、とびとびの値（離散的な値）をとる確率変数についてのものでした。しかし、身長や体重、時間のように連続的な値をとる確率変数もあります。

### 連続確率変数と確率密度

身長などの**連続的な値**をとる確率変数の場合、ある1点の値（例えば「身長がぴったり170.0000...cmである確率」）をとる確率は、ほとんど0になります。なぜなら、取りうる値が無限に近いからです。

そこで、連続的な値の場合は、ある「範囲」を定めて、その範囲に入る確率を考えます。このとき、確率の「濃さ」や「分布の形」を表すのが**確率密度**です。確率密度自体は確率ではありませんが、特定の範囲で積分することでその範囲の確率を求めることができます。

例えば、「データから1人選択したとき、約68.3%は165cm〜175cmである」といった表現は、確率密度の考え方に基づいています。

### 正規分布と確率密度

AIや統計学で最もよく使われる確率分布の一つに**正規分布**があります。正規分布は、平均値を中心として左右対称の釣鐘型（ベルカーブ）の確率密度関数を持ちます。

![正規分布の図](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram_micro.svg/400px-Standard_deviation_diagram_micro.svg.png)
*（画像はイメージです。スライドの図を参考に、平均と標準偏差の範囲を示しています）*

正規分布では、以下の特徴があります。
*   **平均 (μ)** から **±1標準偏差 (σ)** の範囲に、全体の約 **68.3%** のデータが含まれる。
*   **平均 (μ)** から **±2標準偏差 (σ)** の範囲に、全体の約 **95.5%** のデータが含まれる。
*   **平均 (μ)** から **±3標準偏差 (σ)** の範囲に、全体の約 **99.7%** のデータが含まれる。

> 📌 **G検定頻出ポイント**:
> 連続的な確率変数の場合は「確率密度」で考え、特定の範囲に入る確率を求めます。正規分布は自然現象や社会現象で頻繁に見られる分布であり、その特徴（平均と標準偏差によるデータの分布割合）はG検定でも問われやすいポイントです。

---

## 階乗と組み合わせ

### 階乗

**階乗**とは、ある正の整数から1までのすべての自然数を掛け合わせた積のことです。記号「!」を使って表します。

**例**:
*   5! (5の階乗) = 5 × 4 × 3 × 2 × 1 = 120
*   7! (7の階乗) = 7 × 6 × 5 × 4 × 3 × 2 × 1 = 5040

### 組み合わせ

**組み合わせ**とは、異なるn個のものの中から、異なるk個を選ぶ場合の数のことです。選ぶ順番は考慮しません。

**計算式**: C(n, k) = n! / (k! * (n-k)!)

**例**: 異なる5台のスマートフォンの中から、2台のスマートフォンを選ぶ組み合わせの数を求めます。
C(5, 2) = 5! / (2! * (5-2)!)
= 5! / (2! * 3!)
= (5 × 4 × 3 × 2 × 1) / ((2 × 1) × (3 × 2 × 1))
= 120 / (2 × 6)
= 120 / 12
= 10通り

> 📌 **G検定頻出ポイント**:
> 階乗と組み合わせは、確率計算の基礎となる概念です。特に組み合わせは、データの中から特定の条件を満たす要素を選ぶ場合の数を計算する際に用いられます。数式を暗記するよりも、その概念と適用場面を理解することが重要です。

---

## ベルヌーイ分布・二項分布・ポアソン分布

これらは、特定の条件下で事象が起こる回数をモデル化するための重要な確率分布です。

### ベルヌーイ分布

**ベルヌーイ分布**は、結果が2つしかない実験（例えば、コインの裏表、合否、成功・失敗など）である**ベルヌーイ試行**によって得られる確率分布です。

*   成功する確率を `p` とすると、失敗する確率は `1 - p` となります。
*   例えば、コインの表が出る確率が `p = 0.5` のとき、裏が出る確率は `1 - p = 0.5` です。

> 📌 **G検定頻出ポイント**:
> ベルヌーイ分布は、二値分類問題（Yes/No、0/1）の基礎となる分布です。最もシンプルな確率分布の一つであり、他の分布の基礎にもなります。

### 二項分布

**二項分布**は、ベルヌーイ試行を `n` 回繰り返したときに、ある事象（成功）が `k` 回起きる確率を表す確率分布です。

**例**: コインを投げて表が出る確率が `p = 0.5` のとき、5回投げて表が出る回数の確率を考えます。

| 表が出る回数 (k) | 0       | 1        | 2        | 3        | 4        | 5       |
| :--------------- | :------ | :------- | :------- | :------- | :------- | :------ |
| 確率 (%)         | 3.125%  | 15.625%  | 31.25%   | 31.25%   | 15.625%  | 3.125%  |

この表は、5回中2回表が出る確率が最も高いことを示しています。

**計算式**: P(X=k) = C(n, k) * p^k * (1-p)^(n-k)
*   `n`: 試行回数
*   `k`: 事象が起きた回数（成功回数）
*   `p`: 事象が起きる確率（成功確率）
*   `C(n, k)`: `n`回中`k`回成功する組み合わせの数

> 📌 **G検定頻出ポイント**:
> 二項分布は、複数回の試行における成功回数の確率をモデル化します。機械学習では、分類モデルの評価（例：正解率）などに応用されることがあります。

### ポアソン分布

**ポアソン分布**は、ある事象が一定の時間内や空間内で平均して `λ` (ラムダ) 回起こるとき、その事象がちょうど `k` 回起こる確率を表す確率分布です。特に、**稀な事象**が多数の試行の中で発生する回数をモデル化するのに適しています。

**例**: 2,000個に1個の割合 (p = 0.0005) で不良品が発生する工場で、ランダムに1,000個選んだときに不良品が含まれる平均個数を考えます。
*   平均発生回数 `λ = n × p = 1,000 × 0.0005 = 0.5`

この場合、ポアソン分布を使って、1,000個の中に不良品が0個、1個、2個...含まれる確率を計算できます。

**計算式**: P(X=k) = (λ^k * e^(-λ)) / k!
*   `e`: ネイピア数（約2.718）
*   `λ`: 単位時間あたりに事象が発生する平均回数
*   `k`: 事象が発生する回数

**二項分布との関係**:
`n` が大きく `p` が小さい（つまり、試行回数が多く、成功確率が低い）二項分布は、ポアソン分布に近似することができます。これは、稀な事象の発生回数をモデル化する際に、計算が複雑な二項分布の代わりにポアソン分布を用いることができることを意味します。

> 📌 **G検定頻出ポイント**:
> ポアソン分布は、交通量、ウェブサイトへのアクセス数、製品の欠陥数など、稀な事象の発生回数を予測するのに使われます。二項分布との近似関係も重要なポイントです。

---

## 統計的仮説検定

**統計的仮説検定（仮説検定）**とは、ある仮説が正しいかどうかを統計的な手法を用いて検証することです。AIモデルの性能比較や、新しい施策の効果測定など、様々な場面で活用されます。

### 仮説検定の基本的な流れ

1.  **帰無仮説と対立仮説の設定**:
    *   **帰無仮説 (H0)**: 検定を行うための仮説で、**否定したい仮説**である場合が多いです。現状維持や効果がないことを主張します。
    *   **対立仮説 (H1)**: 帰無仮説に対する仮説で、**証明したい仮説**である場合が多いです。新しい効果や変化があることを主張します。

    **例**: コインを100回投げて75回表が出たとき、「このコインは表が出やすい」という仮説を検証する場合。
    *   **帰無仮説 (H0)**: コインで表が出る確率は50%以下である（つまり、表が出やすいとは言えない）。
    *   **対立仮説 (H1)**: コインで表が出る確率は50%より大きい（つまり、表が出やすい）。

2.  **仮説を判断するための数値（有意水準）の設定**:
    **有意水準 (α)** とは、帰無仮説が誤っていると判断する基準となる確率です。一般的に5% (0.05) や1% (0.01) が設定されます。この値が小さいほど、より厳密な判断が求められます。

3.  **検証**:
    収集したデータに基づいて統計量を計算し、その統計量が帰無仮説のもとでどれくらい起こりにくいかを評価します。

4.  **結論**:
    計算された統計量が有意水準よりも「起こりにくい」と判断された場合、帰無仮説を棄却し、対立仮説を採択します。逆に、有意水準よりも「起こりやすい」と判断された場合、帰無仮説を棄却できません（対立仮説を証明できない）。

> 📌 **G検定頻出ポイント**:
> 仮説検定の目的と、帰無仮説・対立仮説、有意水準の役割はG検定で非常に重要です。特に、帰無仮説を「棄却する」「棄却できない」という表現を理解しておきましょう。「帰無仮説を採択する」という表現は通常使いません。

---

## ベクトル・行列

AI、特にディープラーニングでは、データは数値の集まりとして扱われます。この数値の集まりを効率的に扱うために、**ベクトル**や**行列**といった数学的な概念が用いられます。

### ベクトル

**ベクトル**とは、向きと大きさをもった量であり、値を一列に並べたものです。各値や文字のことを**要素**と呼びます。

**例**:
*   `[3, 4]` は2次元ベクトルで、x軸方向に3、y軸方向に4進む向きと大きさを表します。
*   `[身長, 体重, 年齢]` のように、個人の特徴量をまとめたものもベクトルとして表現できます。

### コサイン類似度

**コサイン類似度**は、2つのベクトルの類似度を表す指標です。2つのベクトルが指す方向がどれだけ似ているかを、なす角のコサイン値で評価します。

*   値は **-1 から 1** の範囲をとります。
*   **1** に近いほど、2つのベクトルは同じ方向を向いており、**類似度が高い**ことを示します。
*   **0** に近いほど、2つのベクトルは直交しており、**無関係**であることを示します。
*   **-1** に近いほど、2つのベクトルは真逆の方向を向いており、**類似度が低い（反対の関係）**ことを示します。

AIの分野では、文書の類似度判定（単語の出現頻度をベクトルで表現）、推薦システムなどで活用されます。

### 行列

**行列**とは、値や文字を縦横に並べたものです。複数のベクトルをまとめたものと考えることもできます。

**例**:
```
[ 10  20  30 ]  ← 1行目
[ 40  50  60 ]  ← 2行目
[ 70  80  90 ]  ← 3行目
[100 110 120 ]  ← 4行目
  ↑   ↑   ↑
1列目 2列目 3列目
```
この行列は4行3列の行列です。画像データ（ピクセル値の集まり）や、複数のデータサンプル（各行が1つのデータ、各列が特徴量）を表現する際によく用いられます。

> 📌 **G検定頻出ポイント**:
> ベクトルと行列は、AIにおけるデータ表現の基本です。特にディープラーニングでは、これらの演算がモデルの学習過程で頻繁に行われます。コサイン類似度は、ベクトルの応用として、データの類似性を測る重要な指標です。

---

## 情報量と相互情報量

### 情報量

**情報量**とは、ある出来事が起きたときに、それがどれほど起こりにくいかを表す尺度です。

*   **起こりにくい出来事ほど、情報量は大きくなります。**
*   逆に、必ず起こる出来事や、すでに知っている出来事の情報量は0です。

例えば、「明日の天気は晴れ」という情報と、「明日の天気は雪（熱帯地域で）」という情報を比べると、後者の方が起こりにくいため、情報量が大きいと言えます。AIでは、データの不確実性を測るエントロピーの概念と関連が深いです。

### 相互情報量

**相互情報量**とは、2つの確率変数間の相互依存の度合いを表す指標です。簡単に言えば、「一方の変数を知ることで、もう一方の変数についてどれくらいの情報が得られるか」を示します。

*   **依存度が高い場合**、相互情報量は大きくなります。つまり、一方の変数が分かれば、もう一方の変数も予測しやすくなります。
*   2つの確率変数が**独立しており、互いに影響を与えないとき**、相互情報量は0（または0に近い値）になります。

相互情報量が非常に高い場合は、2つの変数が強い関係を持っていると考えられます。これは、統計学における**相関係数**が高い状態と似た状態であると言えます。特徴量選択など、機械学習の前処理で重要な役割を果たします。

> 📌 **G検定頻出ポイント**:
> 情報量は「起こりにくさ」の尺度、相互情報量は「2つの変数の依存度」の尺度です。特に相互情報量は、特徴量間の関係性を分析する際に用いられ、AIモデルの性能向上に役立つことがあります。

---

## 最尤法

**最尤法（さいゆうほう）**とは、与えられたデータから、そのデータを最もよく説明できるような**最適なパラメータを推定する方法**の一つです。

### 尤度（ゆうど）

最尤法では、「**尤度（ゆうど）**」という概念が重要になります。尤度とは、あるパラメータのときに、**実際に観測されたデータが得られる確率（もっともらしさ）**を示すものです。

*   例えば、あるデータセットがあったときに、
    *   パラメータAの場合の尤度は0.25
    *   パラメータBの場合の尤度は0.75
    と計算されたとします。

この場合、パラメータBの方が、実際に観測されたデータが「より起こりやすい」と説明できるため、**パラメータBが最適なパラメータであると推定する**のが最尤法です。

AIの分野では、機械学習モデルのパラメータ（例：回帰直線の傾きや切片、ニューラルネットワークの重みなど）をデータから学習する際に、この最尤法の考え方が基礎として用いられることがあります。

> 📌 **G検定頻出ポイント**:
> 最尤法は、データからモデルのパラメータを推定する基本的な手法です。「尤度」という言葉と、「尤度が最大になるパラメータを選ぶ」という考え方を理解しておきましょう。

---

## 📝 章末まとめ

この章では、AIに必要な数理・統計知識の後半として、確率の応用、データ表現、そして統計的推論の基礎を学びました。

*   **確率変数**: 試行の結果によって値が決まる変数。
*   **確率分布**: 確率変数の取りうる値と、その値をとる確率の関係。
*   **期待値**: 確率変数の平均的な値。
*   **条件付き確率**: ある事象が起こる条件下で、別の事象が起こる確率。ベイズの定理の基礎。
*   **確率密度**: 連続的な確率変数の「確率の濃さ」。特定の範囲の確率を求める際に使用。
*   **正規分布**: 多くの自然現象に見られる釣鐘型の確率分布。平均と標準偏差で特徴づけられる。
*   **階乗**: ある数から1までの積。
*   **組み合わせ**: 異なるn個からk個を選ぶ場合の数（順序は考慮しない）。
*   **ベルヌーイ分布**: 結果が2つしかない試行（ベルヌーイ試行）の確率分布。
*   **二項分布**: ベルヌーイ試行をn回繰り返したときの成功回数の確率分布。
*   **ポアソン分布**: 稀な事象が一定期間に発生する回数の確率分布。二項分布の近似として用いられる。
*   **統計的仮説検定**: 仮説が正しいかを統計的に検証する手法。
    *   **帰無仮説**: 否定したい仮説。
    *   **対立仮説**: 証明したい仮説。
    *   **有意水準**: 帰無仮説を棄却する判断基準。
*   **ベクトル**: 向きと大きさを持つ量。値を一列に並べたもの。
*   **コサイン類似度**: 2つのベクトルの向きの類似度を示す指標。
*   **行列**: 値を縦横に並べたもの。
*   **情報量**: ある出来事の起こりにくさの尺度。起こりにくいほど情報量大。
*   **相互情報量**: 2つの確率変数間の相互依存の度合い。独立なら0。
*   **最尤法**: 観測データから、尤度（もっともらしさ）が最大となる最適なパラメータを推定する方法。

