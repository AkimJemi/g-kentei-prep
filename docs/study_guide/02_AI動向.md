# 人工知能をめぐる動向

> **G検定 学習ガイド** | 分野：AI動向 | 165P | 2026-02-25

---

## 1. 人工知能の歴史的変遷：全体像

人工知能（AI）の研究は、その時代ごとの技術的制約や社会のニーズに応じて、さまざまなアプローチが試されてきました。ここでは、AI研究の歴史を大きく3つの時代に分けて解説します。

### 1.1. 推論・探索の時代（〜1980年代前半）
この時代は、コンピュータが論理的な推論や探索によって問題を解決しようとした時期です。迷路を解いたり、ボードゲームで最適な手を探索したりする研究が中心でした。コンピュータの計算能力を活かし、与えられたルールの中で最適な解を見つけ出すことに重点が置かれました。

### 1.2. 知識の時代（1980年代前半〜1990年代）
人間が持つ専門知識をコンピュータに与え、それに基づいて推論させる「**エキスパートシステム**」が注目された時代です。特定の分野における専門家の知識を形式化し、コンピュータがその知識を使って診断や判断を行うことを目指しました。

### 1.3. 機械学習・特徴表現学習の時代（2000年代〜現在）
この時代は、データからコンピュータが自ら学習し、特徴を抽出する能力が飛躍的に向上しました。特に**ディープラーニング**の登場により、画像認識や自然言語処理などで目覚ましい成果を上げています。大量のデータからパターンを自動で発見し、予測や分類を行うことが可能になりました。

---

## 2. 推論・探索の時代：コンピュータが問題を解く

この章では、AI研究の初期段階である「推論・探索の時代」にどのような研究が行われてきたかを見ていきましょう。

### 2.1. 探索木とデータ構造

コンピュータが効率的に情報を処理するためには、データの整理方法が重要です。これを**データ構造**と呼びます。適切なデータ構造を選ぶことで、データの格納や検索を高速に行うことができます。

#### 2.1.1. 主要なデータ構造
*   **キュー (Queue)**: 最初に格納したデータから順に取り出す（First-In, First-Out: FIFO）データ構造です。例えば、レジの行列のように、先に並んだ人からサービスを受けるイメージです。
*   **スタック (Stack)**: 最後に格納したデータから順に取り出す（Last-In, First-Out: LIFO）データ構造です。積み重ねたお皿のように、一番上に置いたお皿から順に取るイメージです。
*   **木構造 (Tree Structure)**: 階層を持ったツリー状のデータ構造です。家系図や会社の組織図のように、親と子の関係でデータが繋がっています。
    *   **ノード (Node)**: 木構造を構成する一つ一つの要素です。
    *   **ルート (Root)**: 木構造の一番上のノードです。
    *   **リーフ (Leaf)**: 子を持たない末端のノードです。
    *   **ブランチ (Branch)**: ノード間のつながりを示します。

#### 2.1.2. 探索木とは
**探索木**は、データを効率よく検索するために使用される木構造の一種です。特定のルールに基づいて、目的のデータを探索していきます。これは、コンピュータが大量の選択肢の中から最適な経路やデータを見つけ出すのに非常に得意な分野です。

#### 2.1.3. 探索の種類
探索木を使った探索には、主に以下の2種類があります。

1.  **幅優先探索 (Breadth-First Search: BFS)**
    *   初期のノード（開始点）から見て、近いノードから順に、階層的に探索を行います。
    *   **メリット**: **最短経路**を見つけ出すことができます。例えば、迷路でスタート地点から近い通路を順に調べていくと、必ず最短ルートが見つかります。
    *   **デメリット**: 探索したノードを全て記憶しなければならないため、多くのメモリを消費します。

2.  **深さ優先探索 (Depth-First Search: DFS)**
    *   あるノードから行き止まりになるまで深く探索を進め、行き止まりになったら一つ手前のノードに戻り、別の経路をまた深く探索します。
    *   **メリット**: 記憶する情報が少ないため、メモリ消費を抑えられます。
    *   **デメリット**: 最短経路を見つけられるとは限りません（遠回りする可能性が高いです）。迷路で一つの道をひたすら進み、行き止まったら引き返して別の道を探すイメージです。

> 📌 **G検定頻出ポイント**:
> *   **幅優先探索**は**最短経路**を見つけるのに適していますが、メモリを多く使います。
> *   **深さ優先探索**はメモリ消費が少ないですが、最短経路とは限りません。
> *   それぞれの探索方法の特性を理解し、状況に応じて使い分けることが重要です。

### 2.2. 探索木の応用例：迷路とハノイの塔

探索木は、迷路やハノイの塔のようなシンプルなゲームや問題を解くのに活用できます。重要なのは、問題をコンピュータが処理できる「木構造」の形式に変換できるかどうかです。

*   **迷路**: スタートからゴールまでの経路を、ノード（分岐点や通路）とブランチ（通路）で表現し、探索木を使って解くことができます。
*   **ハノイの塔**: 3本の杭と複数の円盤を使ったパズルゲームです。各円盤の配置状態をノードとし、円盤を移動させる操作をブランチとすることで、探索木として表現し、解法を見つけることが可能です。コンピュータが理解できる形式に問題を落とし込むことで、複雑に見える問題も解決できるようになります。

### 2.3. ロボットの行動計画（プランニング）

ロボットが目標を達成するために取るべき行動を計画することを**行動計画**、または**プランニング**と呼びます。人間が一つ一つ指示するのではなく、ロボット自身が最適な行動パターンを探索して計画を作成することが目指されました。

#### 2.3.1. プランニングの要素
プランニングでは、以下の要素を記述することで、初期状態から目標状態に到達するまでの行動計画を立てます。

*   **前提条件**: ある行動を行うために必要な条件（例: 「本Cが机の上にある」）。
*   **行動**: 状態の変化をもたらす操作（例: 「本Cを棚に移動する」）。
*   **結果**: 行動によって生じる状態の変化（例: 「本Cが棚の中にある」）。

これらの要素を組み合わせて、ロボットは「机の上の本を全て棚の中に移動させる」といった目標を達成するための最適な行動シーケンスを探索します。

#### 2.3.2. STRIPSと積み木の世界
**STRIPS (Stanford Research Institute Problem Solver)** は、プランニングシステムで有名な記述形式です。前提条件、行動、結果を組み合わせて記述することで、コンピュータが行動計画を立てられるようにします。

このプランニングの研究は、**「積み木の世界」**と呼ばれる仮想的な環境で進められました。これは、現実世界の複雑さを単純化し、直方体や円錐などの積み木から成る仮想世界でロボットの行動計画を研究するものです。現実世界はあまりに複雑なため、まずは単純な仮想世界で基礎的な研究が行われました。

#### 2.3.3. SHRDLUとCycプロジェクト
テリー・ウィノグラードは、この「積み木の世界」に存在するブロックを英語の指示で動かすことができるシステム **SHRDLU (シュルドゥル)** を1968年〜1970年に開発しました。これは限定的ではありましたが、自然言語でコンピュータと対話できる画期的なシステムであり、「赤いブロックを持って」「赤いブロックを持ちました」といったやり取りが可能でした。

SHRDLUの研究成果は、後の **Cycプロジェクト (サイクプロジェクト)** に引き継がれていきます。Cycプロジェクトは、人間と同様の推論システムを構築するため、**一般常識**をデータベース化することを目的とした大規模なプロジェクトです。

> 📌 **G検定頻出ポイント**:
> *   **プランニング**はロボットが目標達成のための行動を計画する技術です。
> *   **SHRDLU**は自然言語で積み木を操作するシステムで、初期のAIにおける自然言語処理の重要な成果です。
> *   **Cycプロジェクト**は、人間が持つ膨大な一般常識をコンピュータに教え込むことを目指しました。

### 2.4. ボードゲームと探索戦略

ボードゲームは、AI研究において重要なテーマでした。なぜなら、ボードゲームでは常に最適な手を選択し続けることができれば勝てるため、**探索**と非常に相性が良いからです。

#### 2.4.1. 手の組み合わせの膨大さ
しかし、ボードゲームの手の組み合わせは想像を絶するほど膨大です。

| ゲーム名 | 手の組み合わせの概数 |
| :------- | :------------------- |
| オセロ   | 約10の60乗           |
| チェス   | 約10の120乗          |
| 将棋     | 約10の220乗          |
| 囲碁     | 約10の360乗          |

この天文学的な数の手の中から最適な手を見つけ出すには、膨大な計算が必要となります。例えば、将棋の10の220乗は、宇宙に存在する原子の数よりもはるかに多いと言われています。

#### 2.4.2. 探索の効率化：ヒューリスティックな知識
全ての手を探索するのは現実的ではないため、探索を効率化する工夫が凝らされました。その一つが、**コスト（スコア）**という概念の導入です。

*   それぞれの盤面に対してスコア（点数）を付けます。良い盤面には高いスコア、悪い盤面には低いスコアを設定します。
*   スコアの高い盤面になるような手を選択し、スコアが悪くなるような手は考えないことで、探索範囲を大幅に減らすことができます。

このスコア付けには、人間が経験的に知っている知識が活用されました。これを**ヒューリスティックな知識**と呼びます。
例えば、オセロでは「角に自分のコマがある盤面はスコアが高い」「角に相手のコマがある盤面はスコアが低い」といったルールです。ヒューリスティックな知識を使うことで、全てを計算するよりも短時間で「ある程度正しい答え」を導き出すことが可能になります。

#### 2.4.3. ボードゲームのゲーム戦略：Mini-Max法とαβ法
ボードゲームでは相手の手も考慮に入れる必要があります。自分は有利になる手を選び、相手は自分に不利になる手を選ぶ、という前提で戦略を立てます。

1.  **Mini-Max法 (ミニマックス法)**
    *   数手先の盤面を予測し、その中で自分にとって最も有利な手（最大化：Max）を、相手が最も不利になる手（最小化：Min）を選ぶことを前提に選択する手法です。
    *   しかし、読む手数が1手増えるだけでも計算量は指数関数的に膨れ上がります（例: 1手あたり30通り打てる手がある場合、3手先は30 × 30 × 30 = 27,000通り）。

2.  **αβ法 (アルファベータ法)**
    *   Mini-Max法を改良し、探索の無駄を省くことで効率化した手法です。
    *   「これ以上深く探索しても、より良い手は見つからない」と判断できる局面で、それ以降の探索を**省略（カット）**します。
    *   **αカット**: 相手の手をこれ以上探索する必要がない場合に探索を省略します。
    *   **βカット**: 自分の手をこれ以上探索する必要がない場合に探索を省略します。
    *   これにより、計算量を大幅に削減できます。

    > 📌 **G検定頻出ポイント**:
    > *   **Mini-Max法**は相手の最善手を考慮しつつ自分の最善手を選ぶ戦略です。
    > *   **αβ法**はMini-Max法を効率化したもので、**αカット**と**βカット**によって不要な探索を省略します。

    **Mini-Max法とαβ法の例**:
    Mini-Max法では、考えられる全ての経路を探索

G検定対策の学習書へようこそ！

この章では、AIの基盤となる「知識表現」の手法から、現在のAI技術の主流である「機械学習」「深層学習」、そして最先端の「大規模言語モデル」までを体系的に学びます。それぞれの概念を具体的な例を交えながら、G検定で問われやすいポイントに焦点を当てて解説していきます。

---

## 知識表現の基礎：意味ネットワーク

AIが人間のように知識を扱い、推論を行うためには、まず知識をコンピュータが理解できる形に表現する必要があります。そのための初期の手法の一つが**意味ネットワーク**です。意味ネットワークは、概念（ノード）と概念間の関係（リンク）を図で表現する手法で、人間の思考プロセスを模倣しようとしました。

意味ネットワークでは、主に以下の3つの関係が用いられます。

### 「is-a」の関係（継承関係）

「AはBである」という上位概念と下位概念の関係を表します。
例えば、「車は乗り物である」という場合、車が下位概念、乗り物が上位概念となります。下位概念は上位概念の持つ属性（特性）を受け継ぐという特徴があります。

*   **例**: 「セダンは車である」「車は乗り物である」
    *   この場合、セダンは車の特性（エンジンがある、タイヤがあるなど）を受け継ぎ、さらに車が持つ乗り物の特性（移動手段であるなど）も受け継ぎます。

> 📌 **G検定頻出ポイント**:
> 「is-a」関係は**継承関係**とも呼ばれ、下位概念が上位概念の属性を受け継ぐという特徴を理解しておきましょう。

### 「part-of」の関係（属性関係）

「AはBの一部である」という、全体と部分の関係を表します。
例えば、「タイヤは車の一部である」という場合、タイヤが部分、車が全体となります。

*   **例**: 「エンジンは車の一部である」「指は手の一部である」

### 「has-a」の関係（所有関係）

「AはBを所有している」という、所有者と所有物の関係を表します。
例えば、「車はタイヤを所有している」という場合、車が所有者、タイヤが所有物となります。

*   **例**: 「人間は脳を所有している」「家は窓を所有している」

「part-of」の関係と「has-a」の関係は、視点を変えると真逆の関係と捉えることができます。
*   「タイヤは車の一部である」（part-of）
*   「車はタイヤを所有している」（has-a）

---

## 知識の体系化：オントロジー

意味ネットワークの発展形として、より厳密に知識を体系化しようとするのが**オントロジー**です。

### オントロジーとは

**オントロジー**とは、特定の領域における概念やそれらの概念間の関係を、コンピュータが処理できるように体系的に記述するための学問、方法論、または仕様のことです。

知識をコンピュータで扱う際、作成者によって概念の記述方法が異なると、知識を共有したり再利用したりすることが困難になります。そこで、オントロジーでは概念の記述ルールを明確に定めることで、異なるシステム間での知識の共有や再利用を可能にし、結果として知識構築のコストを全体的に下げることができます。

*   **トム・グルーパー**の「**概念化の明示的な仕様**」という定義が有名です。

> 📌 **G検定頻出ポイント**:
> オントロジーは「**概念化の明示的な仕様**」として、知識の共有と再利用を促進し、知識構築コストを削減する目的で用いられます。

### オントロジーにおける重要な関係

オントロジーでも、意味ネットワークと同様に「is-a」や「part-of」といった関係が重要になります。

#### 「is-a」の関係と推移律

「is-a」の関係には**推移律**が成立するという特徴があります。
推移律とは、「AがBである」かつ「BがCである」ならば、「AはCである」という関係が成立することです。

*   **例**:
    *   「軽自動車は車である」（is-a）
    *   「車は乗り物である」（is-a）
    *   この場合、「軽自動車は乗り物である」という関係も成立します。

#### 「part-of」の関係と推移律

「part-of」の関係は、常に推移律が成立するわけではありません。成立する場合としない場合があります。

*   **推移律が成立する例**:
    *   「エンジンは車の一部である」（part-of）
    *   「車は乗り物の一部である」（part-of）
    *   この場合、「エンジンは乗り物の一部である」という関係も成立します。

*   **推移律が成立しない例**:
    *   「山田はA株式会社に属している（一部である）」（part-of）
    *   「足は山田の一部である」（part-of）
    *   この場合、「足はA株式会社の一部である」という関係は成立しません。

このように、「part-of」の関係は文脈によって意味合いが大きく異なり、実際には5種類以上の意味があるとされています。現状ではこれらを厳密に区別して記述することが難しいという課題があります。

> 📌 **G検定頻出ポイント**:
> 「is-a」関係には推移律が成立しますが、「part-of」関係は常に推移律が成立するとは限らない点を理解しておきましょう。具体的な例で違いを把握することが重要です。

### オントロジーの種類

オントロジーには、知識を構築する思想の違いから、大きく分けて2種類のアプローチがあります。

1.  **ヘビーウェイトオントロジー**:
    *   人間が知識（概念）の関係性を厳密に考え、体系化しようとするアプローチです。
    *   高い厳密性を追求しますが、構築に多大な時間とコストがかかります。
    *   **Cycプロジェクト**が代表例です。

2.  **ライトウェイトオントロジー**:
    *   コンピュータに情報を入力し、自動的に関係性を見つけ出し、効率的に知識を体系化しようとするアプローチです。
    *   厳密性よりも効率性を重視し、大量のデータからパターンを抽出する**ウェブマイニング**や**データマイニング**といった技術と相性が良いです。
    *   **ワトソン**が代表例です。

#### ヘビーウェイトオントロジーの例：Cycプロジェクト

**Cycプロジェクト**は、1984年に開始された、一般常識をデータベース化し、人間と同様の推論システムを構築することを目的としたプロジェクトです。35年以上も入力作業が続けられており、「現代版バベルの塔」とも呼ばれています。膨大な知識を手作業で厳密に記述しようとするヘビーウェイトオントロジーの典型例です。

#### ライトウェイトオントロジーの例：ワトソン

**ワトソン**は、IBMが開発した**質問応答（Question-Answering）システム**です。2011年にはアメリカの人気クイズ番組で人間に勝利し、その能力を世界に示しました。
ワトソンは、Wikipediaなどの大量の情報をライトウェイトオントロジー形式で体系化し、それを活用して質問に対する最も適切な回答を導き出します。ワトソンは質問を「理解」しているわけではなく、統計的な関連性に基づいて回答を生成しているため、IBMはワトソンを「人工知能」ではなく「**拡張知能**」（人間の能力を支援する技術）と呼んでいます。

> 📌 **G検定頻出ポイント**:
> **ヘビーウェイトオントロジー**の代表例は**Cycプロジェクト**、**ライトウェイトオントロジー**の代表例は**ワトソン**です。それぞれの特徴（厳密性 vs 効率性）と目的を理解しておきましょう。ワトソンが「拡張知能」と呼ばれる理由も重要です。

#### 東ロボくんプロジェクト

日本でも、質問応答システム開発の流れとして「**東ロボくん**」プロジェクト（2011〜2016年）がありました。これは、東京大学の入試を突破することを目標にした人工知能プロジェクトです。最終的に東大入試を突破することは不可能と判断され終了しましたが、読解力の重要性や、知識の量だけでは合格が困難であることを示しました（ただし、偏差値57は突破しました）。

#### ウェブマイニングとデータマイニング

*   **ウェブマイニング**: ウェブ上にある膨大なデータを分析し、有益な情報を抽出する手法です。
*   **データマイニング**: 企業が保有する顧客データなど、膨大なデータを分析して有益な情報を抽出する手法です。

これらの技術は、厳密性よりも効率性が求められる場面で活用され、ライトウェイトオントロジーの思想と相性が良いとされています。

### オントロジーの展開：セマンティック・ウェブとLOD

オントロジーの研究は、Web技術と結びつき、**セマンティック・ウェブ**や**LOD（リンクト・オープン・データ）**といった研究へと展開されていきます。

#### セマンティック・ウェブ

**セマンティック・ウェブ**とは、コンピュータがWebページに書かれている情報を自律的に処理（収集・加工など）できるようにするための技術、思想、構想のことです。
現在のWebページは人間が読むことを前提としていますが、セマンティック・ウェブでは、Webページに**メタデータ**（データの情報が記述されたデータ）を付与することで、コンピュータがその内容を理解できるようにします。

*   **例**: 「太宰治」というテキストに `<div property="author">太宰治</div>` のように「author（著者）」というメタデータを付与することで、コンピュータは「太宰治」が「著者」であることを理解できます。

ライトウェイトオントロジーの思想と相性が良く、効率的にWeb上の知識をコンピュータが利用できるようにすることを目指しています。

#### LOD（リンクト・オープン・データ）

**LOD（Linked Open Data）**は、Web上でコンピュータ処理に適したデータを公開・共有するための技術の総称です。セマンティック・ウェブの形成に重要な技術であり、構造化されたデータ同士をリンクさせることで、巨大なデータベースを構築していくことを目指します。これにより、異なる情報源に散らばるデータを統合し、より高度な情報検索や分析が可能になります。

---

## データからの学習：機械学習

AIの進化を大きく牽引しているのが**機械学習**です。

### 機械学習とは

**機械学習（Machine Learning）**は、1959年に**アーサー・サミュエル**によって「明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野」と定義されました。
これは、人間が一つ一つのルールを細かく指示するのではなく、コンピュータが大量のデータ（**学習データ**）をもとにして、自動的にルールやパターンを発見し、学習していくことを指します。

> 📌 **G検定頻出ポイント**:
> **アーサー・サミュエル**による機械学習の定義「**明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野**」は頻出です。

### ルールベースと機械学習の比較

家賃を求めるシステムを例に、ルールベースと機械学習の違いを見てみましょう。

*   **ルールベース**:
    *   専門家が「駅から1km離れたら5,000円安くなる」「築年数が1年長くなるごとに1,000円安くなる」といった具体的なルールや値を設定します。
    *   ルールが明確な場合は有効ですが、複雑な状況や未知のパターンには対応しにくいです。

*   **機械学習**:
    *   専門家が「駅からの距離」「築年数」といった**特徴量**（データを特徴づける要素）を設定します。
    *   その後、大量の過去の物件データ（家賃、駅からの距離、築年数など）をコンピュータに与え、最適な「値」（例：駅から1km離れたら4,100円安くなる、築年数が1年長くなるごとに980円安くなる）を自動的に見つけ出させます。
    *   人間が気づかないような複雑なパターンも学習できる可能性があります。

### 特徴量設計（特徴量エンジニアリング）

機械学習において、どのような**特徴量**を選ぶか、またその特徴量をどのように加工するかは、モデルの精度に大きく影響します。これを**特徴量設計（特徴量エンジニアリング）**と呼びます。

*   **例**: 「駅からの距離」が1.2kmや800mなど単位がバラバラの場合、モデルが学習しにくいことがあります。これを1200m、800mのように単位を統一したり、適切なスケールに変換したりすることで、モデルの学習効率や精度が向上します。

ディープラーニングなどの高度な機械学習では、特徴量自体をモデルが自動的に学習していくこともあります。

### データ量の増加と機械学習の発展

インターネットの普及により、Web上には膨大なデータ（**ビッグデータ**）が蓄積されるようになりました。これにより、大量のデータを収集し、機械学習モデルを学習させることが可能になり、モデルの精度が飛躍的に向上しました。学習させるデータ量が多いほど、一般的にモデルの精度は上がるとされています。

### パターン認識

機械学習は、もともと**パターン認識**の分野の技術として発展してきました。
**パターン認識**とは、画像や音声、テキストなど様々なデータから、一定のパターンや特徴を見つけ出し、識別・分類する技術のことです。

*   **例**: 音声認識（話している内容をテキスト化）、顔認識（画像から人物の顔を識別）、文字認識（手書き文字をデジタルデータ化）など。

機械学習を活用することで、人間が発見できなかったような複雑なパターンを見つけることができる可能性もあります。

### 機械学習の応用例

機械学習は私たちの日常生活の様々な場面で活用されています。

*   **スパムフィルター**: 受信したメールが迷惑メールかどうかを自動的に判断するシステムです。過去の迷惑メールのパターンを学習し、新たなメールを分類します。
*   **レコメンデーションエンジン**: ユーザーの過去の行動（購入履歴、閲覧履歴など）や興味を学習し、興味がありそうな商品や情報を提案するシステムです。
    *   **例**: Amazonの「おすすめ商品」、YouTubeの「おすすめ動画」など。

### みにくいアヒルの子定理

**みにくいアヒルの子定理**とは、「何らかの仮定に基づいて特徴量を選択しなければ、分類することは不可能である」という考え方です。
データはそれぞれ似ているようで、完全に同じではありません。何かを分類しようとするとき、私たちは無意識のうちに「どの特徴に注目するか」という仮定を置いています。

| 項目         | 毛色   | 生まれ月 | 体重  | 性別 |
| :----------- | :----- | :------- | :---- | :--- |
| 醜いアヒルの子 | 黒色   | 4月      | 110g  | オス |
| アヒルの子A  | 黄色   | 4月      | 95g   | オス |
| アヒルの子B  | 黄色   | 4月      | 110g  | メス |
| アヒルの子C  | 黄色   | 3月      | 105g  | オス |

上記の表のように、アヒルの子たちはそれぞれ異なる特徴を持っています。これらを「アヒルの子」と「醜いアヒルの子」に分類するためには、「毛色」に注目するという仮定を置く必要があります。もし「体重」や「性別」に注目したら、異なる分類結果になるでしょう。この定理は、機械学習における特徴量選択の重要性を示唆しています。

> 📌 **G検定頻出ポイント**:
> **みにくいアヒルの子定理**は、分類を行うためには、何らかの仮定に基づいて特徴量を選択する必要があることを示しています。特徴量選択の重要性を問われることがあります。

### 自然言語処理の発展

インターネットの普及は、Webページの増加とともに、大量のテキストデータ（自然言語）を生み出しました。これにより、テキストデータの収集コストが低下し、**自然言語処理**のモデル開発がさらに盛んになりました。

**自然言語処理（Natural Language Processing: NLP）**とは、人が日常的に使っている言語（自然言語）をコンピュータで処理させる技術のことです。

*   **例**: 言語翻訳、検索エンジン、文章要約、感情分析など。

私たちが検索エンジンで欲しい情報を見つけたり、翻訳サービスを手軽に利用できたりするのは、自然言語処理の能力が向上したおかげです。

#### 統計的自然言語処理

自然言語処理の精度向上に大きく貢献したのが、**統計的自然言語処理**です。これは、確率論や統計学を使って自然言語を処理するアプローチです。

*   **コーパス（対訳データ）**: 大量の自然言語の文章を構造化したもので、イメージとしては大量の例文が集められたデータベースです。
*   **例**: 従来の機械翻訳は単語を一つ一つ訳していましたが、コーパスを活用することで、文脈に応じたより自然で正確な翻訳が可能になりました。

日本語の「はし」のように、「箸」「橋」「端」など複数の意味を持つ単語（**多義語**）も、前後の単語や文脈を統計的に分析することで、正しい意味を予測し、適切に翻訳できるようになりました。例えば、「はし」の近くに食べ物に関する記述があれば「箸」と予測し、「Chopsticks」と翻訳するといった具合です。

---

## AIの深層へ：深層学習

機械学習の一種であり、現在のAIブームの火付け役となったのが**深層学習**です。

### ニューラルネットワークの基礎

**深層学習（ディープラーニング）**は、人間の脳の神経回路（**ニューロン**）を模倣した数理モデルである**ニューラルネットワーク**を多層化したものです。
ニューロンは、他のニューロンから信号を受け取り、その信号が一定の閾値を超えると、次のニューロンに信号を送るという仕組みを持っています。

初期のニューラルネットワークとしては、1958年に提案された**単純パーセプトロン**などがあります。

### ディープラーニングのブレイクスルー

ディープラーニングが世界的に注目されるきっかけとなったのは、2012年の**ILSVRC（ImageNet Large Scale Visual Recognition Challenge）**という画像認識の精度を競う国際的な大会でした。
この大会で、トロント大学の**ジェフリー・ヒントン**が中心となった「**SuperVision**」チームが、**AlexNet**というディープラーニングモデルを用いて圧倒的な精度で優勝しました。当時、画像認識には従来の機械学習手法が主流でしたが、ディープラーニングがその性能を大きく上回ることを示しました。

2015年以降には、ディープラーニングの認識精度が人間の認識率を超えるまでになりました。ディープラーニングのアイデア自体は以前から存在していましたが、計算能力の向上や大量のデータが利用可能になったことで、技術的に実現可能となり、一躍脚光を浴びることになったのです。

> 📌 **G検定頻出ポイント**:
> **ILSVRC 2012**で**AlexNet**が優勝し、**ジェフリー・ヒントン**がディープラーニングのブレイクスルーに貢献したことは頻出です。ディープラーニングが人間の認識率を超えた時期（2015年以降）も覚えておきましょう。

### 特徴表現学習

従来の機械学習では、人間が「駅からの距離」や「築年数」といった**特徴量**を設計する必要がありました。しかし、ディープラーニングでは、モデルが大量のデータから、適切な出力をするために重要な要素（特徴量）自体を自動的に学習します。これを**特徴表現学習**と呼びます。

*   **例**: 家賃を求めるシステムの場合、ディープラーニングモデルは、人間が意識しないような複雑な要素（例：周辺の騒音レベル、日当たりと家賃の関係性など）を自ら特徴量として抽出し、学習することができます。

特徴表現学習は、データから重要な要素を抽出し、それをコンピュータが処理しやすい形式（ベクトルなど）で表現することを学習する**表現学習**の一種です。

### 人工知能を導入するときの注意点

人工知能（AI）は強力なツールですが、導入する際にはいくつかの注意点があります。

1.  **人工知能の導入が最適なのかを考える**:
    *   AIは万能ではありません。ルールが明確で、比較的単純な問題であれば、ルールベースのシステムの方が安価で効率的に解決できる場合があります。AIはあくまで問題解決のための「手段」の一つに過ぎません。

2.  **業務プロセスを見直す**:
    *   AIを導入する際は、既存の業務プロセスがAIと連携しやすいように見直すことが重要です。AIの能力を最大限に引き出すためには、組織や業務フローの変更が必要になることもあります。

3.  **効果を測定する**:
    *   AI導入によってどのような効果（コスト削減、効率向上、新たな価値創造など）があったのかを具体的に測定し、評価することが大切です。これにより、AI導入が本当にベストな選択だったのかを判断し、改善につなげることができます。

---

## 次世代のAI：大規模言語モデル

近年、AI分野で最も注目を集めているのが**大規模言語モデル（LLM）**です。

### 大規模言語モデル（LLM）とは

**大規模言語モデル（Large Language Model: LLM）**とは、インターネット上の膨大なテキストデータ（数千億〜数兆単語規模）で学習し、自然言語処理に関する様々なタスク（文章生成、翻訳、要約、質問応答など）を高い精度でこなすことができるモデルのことです。

*   **ChatGPT**は、OpenAIが開発した大規模言語モデルである**GPT**をベースに作られた対話型AIサービスです。
    *   **ChatGPT**: サービス名（対話型AI）
    *   **GPT**: 大規模言語モデルの名称

LLMは、人間が書いた文章と区別がつかないレベルの自然な文章を生成できるようになっており、その生成速度は人間の執筆速度をはるかに超えます。
LLMは、与えられた文脈に基づいて「次に来る単語の出現確率」を予測し、最も確率の高い単語を順に選択していくことで文章を生成しています。

*   **例**: 「今日の天気は」という入力に対し、LLMは「晴れ」が98.7%、「コップ」が0.2%、「休み」が1%、「コーヒー」が0.1%といった確率を計算し、最も確率の高い「晴れ」を選択して文章を続けます。

> 📌 **G検定頻出ポイント**:
> **大規模言語モデル（LLM）**は、大量のテキストデータで学習し、自然言語処理タスクをこなすモデルです。**ChatGPT**はサービス名、**GPT**はモデル名である点を区別して理解しておきましょう。

### トランスフォーマーと事前学習・ファインチューニング

近年のLLMの精度向上に大きく貢献した技術が、Googleが開発した**トランスフォーマー（Transformer）**と呼ばれるニューラルネットワークのアーキテクチャです。GPT（Generative Pre-trained Transformer）の「T」はこのトランスフォーマーを指しています。

LLMなどのモデルは、まず大量のデータを用いて汎用的な知識やパターンを学習します。このプロセスを**事前学習（Pre-training）**と呼びます。
事前学習を終えたモデルは、特定のタスク（例：特定の業界の文章生成、顧客対応チャットボットなど）に合わせて、少量のデータで追加学習を行うことで、そのタスクに特化した高精度のモデルを作成できます。この追加学習のプロセスを**ファインチューニング（Fine-tuning）**と呼びます。

ファインチューニングの利点は、ゼロからモデルを構築するよりもはるかにコストと時間を抑えられる点にあります。Googleなどが提供する事前学習済みモデルを利用し、自分たちの用途に合わせてファインチューニングを行うことで、効率的に高性能なAIモデルを開発することが可能です。

### 生成AIとしてのLLM

LLMは、新しい文章や画像、音声などを生成するAIである「**生成AI（Generative AI）**」の一種です。LLMはテキスト生成に特化していますが、他にも画像生成AI（例：DALL-E, Midjourney）など、様々な生成AIが登場しています。

---

## 📝 章末まとめ

*   **意味ネットワーク**: 概念と関係を図で表現する知識表現手法。
    *   **is-a関係**: 継承関係。下位概念が上位概念の属性を受け継ぐ。
    *   **part-of関係**: 全体と部分の関係。
    *   **has-a関係**: 所有者と所有物の関係。
*   **オントロジー**: 概念を体系化し、知識の共有・再利用を可能にする学問・方法論。
    *   **トム・グルーパー**: 「概念化の明示的な仕様」と定義。
    *   **is-a関係**: 推移律が成立する。
    *   **part-of関係**: 推移律が常に成立するとは限らない。
    *   **ヘビーウェイトオントロジー**: 厳密性を追求。例: **Cycプロジェクト**。
    *   **ライトウェイトオントロジー**: 効率性を追求。例: **ワトソン**（質問応答システム、**拡張知能**）。
    *   **セマンティック・ウェブ**: コンピュータがWeb情報を理解・処理するための技術。**メタデータ**が重要。
    *   **LOD（リンクト・オープン・データ）**: 構造化データをWeb上で公開・共有する技術。
*   **機械学習**: 明示的なプログラムなしにデータから学習する技術。
    *   **アーサー・サミュエル**: 機械学習の定義者。
    *   **特徴量設計（特徴量エンジニアリング）**: モデル精度に影響する特徴量の選択・加工。
    *   **パターン認識**: 画像や音声などからパターンを識別する技術。
    *   **みにくいアヒルの子定理**: 分類には仮定に基づく特徴量選択が必要。
    *   **自然言語処理（NLP）**: 人間が使う言語をコンピュータで処理する技術。
    *   **統計的自然言語処理**: 確率論・統計学を用いたNLP。**コーパス**を活用。
*   **深層学習（ディープラーニング）**: ニューラルネットワークを多層化した機械学習の一種。
    *   **ニューラルネットワーク**: 人間の神経回路を模倣した数理モデル。
    *   **ILSVRC 2012**: 画像認識大会。**AlexNet**が優勝し、ディープラーニングが注目されるきっかけに。
    *   **ジェフリー・ヒントン**: ディープラーニングの発展に貢献。
    *   **特徴表現学習**: モデルが自動的に特徴量を学習する。
    *   AI導入時の

