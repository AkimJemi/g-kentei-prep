# 機械学習の具体的手法

> **G検定 学習ガイド** | 分野：機械学習 | 232P | 2026-02-25

---

機械学習は、データからパターンやルールを学習し、未知のデータを予測したり、分類したりする技術です。この章では、機械学習の主要な手法と、それぞれの具体的なアルゴリズムについて詳しく解説します。

## 1. 機械学習の学習の種類

機械学習の手法は、大きく分けて3種類あります。解きたい問題の種類や利用できるデータの形式に合わせて、適切な手法を選択することが重要です。

### 1.1 教師あり学習

**教師あり学習**とは、**特徴量**（入力データ）とそれに対応する**正解ラベル**（出力データ、答え）がセットになったデータを用いて、ルールやパターンを学習していく手法です。例えるなら、先生（正解ラベル）から答えを教えてもらいながら問題を解く（学習する）ようなイメージです。

学習したモデルは、未知のデータが与えられたときに、そのデータの特徴から正解ラベルを予測できるようになります。

#### 1.1.1 教師あり学習で解ける問題

教師あり学習は、主に以下の2種類の問題解決に用いられます。

*   **分類問題**: データが属する**カテゴリ（離散値）**を予測する問題です。
    *   **例**:
        *   メールが「迷惑メール」か「一般のメール」かを分類する。
        *   画像から「犬」か「猫」かを分類する。
        *   顧客を「優良顧客」「一般顧客」などに分類する。
    *   あらかじめ決められた複数のカテゴリの中から、どれに当てはまるかを予測します。

*   **回帰問題**: **連続値**（数値）を予測する問題です。
    *   **例**:
        *   気温や湿度からビールの売上を予測する。
        *   駅からの距離や床面積から家賃を予測する。
        *   過去の株価データから明日の株価を予測する。
    *   予測される値は、体重や温度のように、値と値の間に無限の値を取りうる連続的な数値です。

#### 1.1.2 連続値と離散値

*   **連続値**: 体重（60kg、60.1kg、60.001kgなど）や温度のように、値と値の間に無限の値が存在するデータです。
*   **離散値**: クラス番号（1組、2組）やカテゴリ（男性、女性）のように、値が飛び飛びで連続していないデータです。

> 📌 **G検定頻出ポイント**:
> *   **教師あり学習**: 正解ラベル付きデータを使用。
> *   **分類問題**: 離散値（カテゴリ）を予測。
> *   **回帰問題**: 連続値（数値）を予測。
> *   この3つの概念とそれぞれの具体例はG検定で頻繁に問われます。

### 1.2 教師なし学習

**教師なし学習**とは、**正解ラベルが付与されていないデータ**（特徴量のみ）をもとに、データの中に潜むルールやパターンを自律的に学習していく手法です。先生がいない状態で、自分でデータの構造や関係性を見つけ出すようなイメージです。

#### 1.2.1 教師なし学習で解ける問題

教師なし学習は、主に以下の問題解決に用いられます。

*   **クラスタリング**: データを**類似度**に基づいていくつかの**クラスタ（グループ）**に分類する手法です。
    *   **例**:
        *   購入データから顧客をいくつかのグループに分類し、それぞれのグループの特性を分析する。
        *   記事の文章を内容の類似性に基づいて自動で分類する。
    *   教師あり学習の分類問題と異なり、事前にカテゴリが決まっているわけではなく、データの特徴から自動的にグループが形成されます。

*   **次元削減（次元圧縮）**: データの**特徴量を要約**し、扱うデータの数を減らす手法です。
    *   **例**:
        *   「数学」「理科」の成績を「理系」という一つの特徴にまとめる。
        *   「社会」「国語」の成績を「文系」という一つの特徴にまとめる。
    *   データの持つ意味を保ちながら、不要な情報や重複する情報を削減することで、計算量を減らしたり、データの可視化を容易にしたりする目的で使われます。

> 📌 **G検定頻出ポイント**:
> *   **教師なし学習**: 正解ラベルなしデータを使用。
> *   **クラスタリング**: 類似度に基づき自動でグループ分け。
> *   **次元削減**: データの特徴量を要約し、数を減らす。
> *   クラスタリングと分類問題の違い（事前カテゴリの有無）は重要です。

### 1.3 半教師あり学習

**半教師あり学習**は、**教師なし学習と教師あり学習を組み合わせた**手法です。少量の正解ラベル付きデータと、大量の正解ラベルなしデータを併用して学習を進めます。

#### 1.3.1 半教師あり学習の利用場面

*   **利用場面**: 大量の正解ラベル付きデータを収集することが難しい場合や、正解ラベルを一つひとつ手作業で付与するコストを削減したい場合に有効です。
*   **特徴**: 一般的に、教師あり学習のみの場合と比較して精度は低くなる傾向がありますが、コスト面で大きなメリットがあります。

### 1.4 強化学習

**強化学習**は、与えられた**環境下で、エージェント（自律的に行動するシステムやプログラム）が試行錯誤を繰り返し、報酬が最大となるような最適な行動を学習していく**手法です。

#### 1.4.1 強化学習の仕組み

*   **エージェント**: 環境内で行動する主体（例：お掃除ロボット、ゲームAI）。
*   **環境**: エージェントが行動する場。
*   **行動**: エージェントが環境に対して行う操作。
*   **報酬**: エージェントの行動を評価するスコア。良い行動には高い報酬、悪い行動には低い報酬が与えられます。
*   **学習**: エージェントは、より多くの報酬を得られるように行動戦略を改善していきます。

#### 1.4.2 強化学習の具体例

*   **お掃除ロボット**: 部屋を効率的に掃除し、バッテリー切れを避けるような行動を学習します。
*   **囲碁や将棋のAI**: 勝利という報酬を目指して、最適な手を打つ戦略を学習します。
*   **自動運転**: 目的地に安全かつ効率的に到達するための運転行動を学習します。

> 📌 **G検定頻出ポイント**:
> *   **強化学習**: 環境、エージェント、行動、報酬の概念を理解する。
> *   試行錯誤を通じて最適な行動を学習する点が特徴。
> *   ゲームAIやロボット制御など、具体的な応用例も押さえる。

---

## 2. 教師あり学習の具体的手法

ここからは、教師あり学習の代表的なアルゴリズムを詳しく見ていきましょう。

### 2.1 線形回帰

**線形回帰**は、**目的変数**（予測したい変数）を1つ以上の**説明変数**（原因となる変数）を使って予測するための手法です。説明変数と目的変数の間に線形（直線的）な関係があると仮定してモデルを構築します。

#### 2.1.1 目的変数と説明変数

*   **目的変数 (y)**: 予測したい変数、結果となる変数（例：アイスクリームの売上）。
*   **説明変数 (x)**: 目的変数を説明するための変数、原因となる変数（例：気温）。

**例**: 気温からアイスクリームの売上を予測したい場合
*   目的変数：アイスクリームの売上
*   説明変数：気温

#### 2.1.2 単回帰分析と重回帰分析

線形回帰には、説明変数の数によって以下の2種類があります。

| 種類         | 説明変数の数 | 特徴                                     | 例                                     |
| :----------- | :----------- | :--------------------------------------- | :------------------------------------- |
| **単回帰分析** | 1つ          | 1つの説明変数で1つの目的変数を予測する。 | 気温からアイスクリームの売上を予測する。 |
| **重回帰分析** | 複数         | 複数の説明変数で1つの目的変数を予測する。 | 人口、気温などからアイスクリームの売上を予測する。 |

#### 2.1.3 線形回帰のモデル式

*   **単回帰分析のモデル式**: $y = ax + b$
    *   $y$: 目的変数
    *   $x$: 説明変数
    *   $a$: **傾き（回帰係数）** - $x$が1単位増えたときに$y$がどれだけ変化するかを示す。
    *   $b$: **切片（回帰定数）** - $x$が0のときの$y$の値。

*   **重回帰分析のモデル式**: $y = a_1x_1 + a_2x_2 + \dots + a_nx_n + b$
    *   $x_1, x_2, \dots, x_n$: 複数の説明変数
    *   $a_1, a_2, \dots, a_n$: それぞれの説明変数に対応する**偏回帰係数**
    *   偏回帰係数の大きさは、その説明変数が目的変数の予測にどれだけ影響を与えているかを示します。

#### 2.1.4 最小二乗法

線形回帰では、実際の値と予測値との差（残差）の2乗和が最小になるように、回帰係数（$a$）と切片（$b$）を決定します。この方法を**最小二乗法**と呼びます。

#### 2.1.5 多重共線性

**重回帰分析**を行う際に注意すべき点として、**多重共線性**があります。
*   **多重共線性**: 複数の説明変数（特徴量）の間に高い相関関係が存在すること。
*   **問題点**: 計算が不安定になったり、個々の説明変数の影響度（偏回帰係数）が正しく評価できなくなったり、予測精度が低下したりする可能性があります。

#### 2.1.6 正則化

**正則化**は、線形回帰モデルが**過学習**に陥るのを防ぐための手法の一つです。
*   **過学習**: 訓練データに対してモデルが過度に適応しすぎてしまい、訓練データ以外の未知のデータに対する予測精度が悪くなる現象。例えるなら、テスト範囲の問題は完璧に解けるが、少し形式が変わると全く解けなくなる状態です。
*   **正則化の種類**:
    *   **ラッソ回帰**: 不要な説明変数の係数を0に近づけ、モデルを簡素化する。
    *   **リッジ回帰**: 各説明変数の係数を小さくすることで、モデルの複雑さを抑える。

> 📌 **G検定頻出ポイント**:
> *   **線形回帰**: 連続値予測（回帰問題）に利用。
> *   **目的変数・説明変数**: それぞれの役割を理解する。
> *   **単回帰・重回帰**: 説明変数の数による違い。
> *   **最小二乗法**: 係数決定の原理。
> *   **多重共線性**: 重回帰分析における注意点。
> *   **正則化・過学習**: 過学習の概念と正則化による対策。

### 2.2 ロジスティック回帰

**ロジスティック回帰**は、ある事象が発生する**確率を予測**する手法です。線形回帰と名前は似ていますが、主に**分類問題**に用いられます。

#### 2.2.1 確率予測と分類への応用

*   ロジスティック回帰は、線形回帰の出力（$-\infty$から$+\infty$までの連続値）を**シグモイド関数**という特殊な関数に通すことで、0から1の間の確率値に変換します。
*   この確率値を利用して分類を行います。例えば、「病気の発症確率が50%以上なら再検査が必要、50%未満なら不要」といったように、**閾値（基準とする値）**を設定することで、データを2つのカテゴリに分類できます。

#### 2.2.2 閾値の設定

*   一般的に、閾値は0.5が基本とされますが、問題の状況によって調整が必要です。
*   **例**: スパムメールの判定では、誤って重要なメールをスパムと判断する（誤検知）リスクを避けるため、閾値を高く設定することがあります。

#### 2.2.3 シグモイド関数とソフトマックス関数

*   **シグモイド関数**: 結果が2択（**2値分類**）の場合に使用されます。
    *   **例**: 再検査が必要か不要か、合格か不合格か。
*   **ソフトマックス関数**: 結果が3択以上（**多クラス分類**または**マルチクラス分類**）の場合に使用されます。
    *   **例**: 合格判定（A判定、B判定、C判定）、服のサイズ（S, M, L）。

> 📌 **G検定頻出ポイント**:
> *   **ロジスティック回帰**: 確率予測、主に**分類問題**に利用。
> *   **シグモイド関数**: 2値分類。
>   **ソフトマックス関数**: 多クラス分類。
> *   閾値の概念と、その設定が分類結果に与える影響。

### 2.3 決定木

**決定木**は、ツリー（樹形図）構造を用いてデータを分析し、予測や分類を行う手法です。データの特徴量に基づいて質問を繰り返し、最終的な結論（予測値やカテゴリ）にたどり着きます。

#### 2.3.1 ツリー構造の要素

*   **根ノード（ルートノード）**: ツリーの最上位にある最初の分岐点。
*   **中間ノード**: 途中の分岐点。
*   **葉ノード（リーフノード）**: ツリーの末端にある、最終的な予測結果や分類カテゴリを示すノード。

#### 2.3.2 回帰木と分類木

*   **回帰木**: 回帰問題（連続値の予測）に用いられる決定木です。
    *   **例**: 駅からの距離や築年数から家賃を予測する。
*   **分類木**: 分類問題（カテゴリの予測）に用いられる決定木です。
    *   **例**: 購入頻度や購入単価から顧客を「優良顧客」「リピーター」などに分類する。

#### 2.3.3 分岐と過学習

決定木は、データを「はい」か「いいえ」のような質問で分割していくことでツリーを形成します。
*   **過学習**: 分岐を深くしすぎると、訓練データに過度に適応し、個々のデータに特化したルールが作られてしまいます。その結果、未知のデータに対する予測精度が低下する**過学習**が発生しやすくなります。
*   **対策**: 過学習を防ぐためには、木の深さや幅を適切に制限する（**剪定**）必要があります。

#### 2.3.4 情報利得と不純度

決定木がデータを分割する際、どの特徴量で、どの閾値で分割すれば最も効率的かを判断するために、**情報利得**や**不純度**といった指標が使われます。
*   **不純度**: 1つのノードに異なるクラスのサンプルがどれだけ混ざっているかを示す割合です。不純度が低いほど、そのノードは「純粋」であると言えます。
    *   不純度の計算には、**エントロピー**や**ジニ係数**がよく用いられます。
*   **情報利得**: 分割前の不純度から分割後の不純度を引いた値です。情報利得が最大になるように分割することで、効率的にデータを分類できます。

#### 2.3.5 決定木の特徴

*   **高い解釈性**: ツリー構造が視覚的に分かりやすく、どの特徴量が予測に重要であるか、どのようなルールで分類・予測が行われているかを人が容易に理解できます。
    *   **例**: 「購入回数」が「利益」に大きく影響すると分かれば、購入回数を増やす施策を検討できます。
*   **変更の容易性**: モデルのルール（特徴量や閾値）を後から人が変更しやすいという利点があります。
*   **外れ値への弱さ**: 外れ値が多いと、不適切な分割が行われることがあります。データ量を増やすことで、ある程度は解決できます。

> 📌 **G検定頻出ポイント**:
> *   **決定木**: ツリー構造、高い解釈性。
> *   **回帰木・分類木**: それぞれの用途。
> *   **過学習**: 決定木で起こりやすい現象と剪定による対策。
> *   **情報利得・不純度**: 分岐の基準となる指標。エントロピー、ジニ係数。

### 2.4 ランダムフォレスト

**ランダムフォレスト**は、複数の決定木を組み合わせることで、単一の決定木よりも高い予測精度と安定性を実現する**アンサンブル学習**の手法の一つです。

#### 2.4.1 ランダムフォレストの仕組み

1.  **複数の決定木の作成**: 訓練データから無作為に一部を抽出し（**ブートストラップサンプリング**）、さらに特徴量もランダムに選択して、複数の決定木を独立して学習させます。
2.  **予測の統合**:
    *   **分類問題の場合**: 各決定木が予測した結果に対して**多数決**を取り、最も多いクラスを最終的な出力とします。
    *   **回帰問題の場合**: 各決定木が予測した結果の**平均値**を最終的な出力とします。

#### 2.4.2 多様性の確保

*   **訓練データのランダム抽出**: 各決定木が異なるデータセットで学習することで、決定木ごとの多様性が生まれます。
*   **特徴量のランダム抽出**: 各決定木が学習に使う特徴量もランダムに選択することで、決定木同士の構造が似すぎず、それぞれが異なる視点から学習します。これにより、個々の決定木の精度が低くても、全体として高い精度が得られます。

#### 2.4.3 ブートストラップサンプリングと復元抽出

*   **ランダムサンプリング**: 無作為に一部の訓練データを抽出すること。
*   **ブートストラップサンプリング**: サンプルを抽出する際に、**重複を許して**無作為に一部のデータを抽出する方法です。これにより、各決定木に多様なデータセットが与えられ、過学習を防ぐ効果もあります。
    *   **復元抽出**: サンプルを抽出した後、それを元に戻してから次のサンプルを抽出する方法。ブートストラップサンプリングで用いられます。
    *   **非復元抽出**: サンプルを抽出した後、それを元に戻さずに次のサンプルを抽出する方法。

#### 2.4.4 ランダムフォレストの特徴

*   **高い予測精度**: 複数の決定木を組み合わせることで、単一の決定木よりも高い予測精度を発揮します。
*   **過学習に強い**: 個々の決定木が過学習していても、全体として結果を統合するため、その影響を受けにくく、過学習が発生しにくい傾向があります。
*   **特徴量重要度の算出**: 各決定木の学習結果を分析することで、どの特徴量が予測にどれだけ重要であったかを算出できます。

#### 2.4.5 アンサンブル学習の種類

ランダムフォレストは、複数の学習器を組み合わせて予測を行う**アンサンブル学習**の一種です。アンサンブル学習には、主に以下の2つのアプローチがあります。

*   **バギング (Bagging)**:
    *   複数のモデルを**並列的**に学習させる手法です。
    *   各モデルは独立して学習し、その結果を平均したり多数決で統合します。
    *   ランダムフォレストはバギングの一種であり、学習時間が比較的短いという特徴があります。

*   **ブースティング (Boosting)**:
    *   複数のモデルを**直列的**に学習させる手法です。
    *   前のモデルが誤分類したデータや予測を間違えたデータに重みをつけたり、残差を学習したりして、次のモデルがその間違いを修正するように学習を進めます。
    *   **特徴**:
        *   バギングよりも予測精度が高い傾向がありますが、直列処理のため学習に時間がかかります。
        *   過学習になりやすい傾向があり、モデルのチューニングが難しい場合があります。
    *   **代表的な手法**:
        *   **勾配ブースティング**: 勾配降下法を用いて、前のモデルの予測誤差を次のモデルが学習します。
        *   **XGBoost (Extreme Gradient Boosting)**: 勾配ブースティングを改良し、高速かつ高精度なことで知られる手法です。決定木と勾配ブースティングを組み合わせたものです。
        *   **AdaBoost (Adaptive Boosting)**: 誤分類したデータの重みを大きくして、次のモデルがそのデータに注目するように学習を進めます。

> 📌 **G検定頻出ポイント**:
> *   **ランダムフォレスト**: 複数の決定木、多数決/平均、**アンサンブル学習（バギング）**の一種。
> *   **ブートストラップサンプリング**: 復元抽出によるデータ抽出方法。
> *   **バギングとブースティングの違い**: 並列/直列、学習時間、精度、過学習への傾向。
> *   **ブースティングの代表的手法**: XGBoost, AdaBoost, 勾配ブースティング。

### 2.5 サポートベクターマシン (SVM)

**サポートベクターマシン (SVM)** は、主に**分類問題**で用いられる強力なアルゴリズムです。異なるクラスのデータを最もきれいに分離する「境界線（超平面）」を見つけることを目指します。

#### 2.5.1 マージン最大化

SVMの基本的な考え方は**マージン最大化**です。
*   **マージン**: 分類する境界線（超平面）から、各クラスに最も近いデータ点（**サポートベクトル**）までの距離のことです。
*   **マージン最大化**: 異なるクラスのデータを分離する境界線は無数に考えられますが、SVMは、その中でマージンが最大になるような境界線を見つけようとします。マージンを最大化することで、未知のデータに対しても汎化性能の高い（頑健な）分類が可能になります。

#### 2.5.2 ハードマージンとソフトマージン

*   **ハードマージン**: 訓練データにおいて、一切の誤分類を許さずにマージンを最大化する方法です。データが完全に線形分離可能な場合に有効ですが、外れ値に非常に敏感です。
*   **ソフトマージン**: 訓練データにおいて、多少の誤分類やマージン内へのデータ点の侵入を許容しながらマージンを最大化する方法です。現実のデータはノイズや外れ値を含むことが多いため、ソフトマージンが一般的に用いられます。
    *   どの程度誤分類を許容するかは、**スラック変数**というパラメータで調整します。

#### 2.5.3 カーネル法（カーネルトリック）

SVMは、もともと直線で分類できるデータ（線形分離可能なデータ）に強い手法ですが、**カーネル法**を組み合わせることで、直線では分類できない複雑なデータ（非線形分離可能なデータ）も分類できるようになります。
*   **カーネル法**: データをより高次元の空間に写像（変換）することで、元の空間では非線形分離不可能だったデータを、高次元空間では線形分離可能にする手法です。
*   **カーネル関数**: 次元を拡張させるための関数です。
*   **カーネルトリック**: 高次元空間での計算を実際に行うことなく、元の低次元空間での計算で高次元空間での内積を計算できるテクニックです。これにより、計算コストを抑えながら高次元化の恩恵を受けられます。

> 📌 **G検定頻出ポイント**:
> *   **SVM**: 主に分類問題、**マージン最大化**の概念。
> *   **サポートベクトル**: 境界線に最も近いデータ点。
> *   **ハードマージン・ソフトマージン**: 誤分類の許容度による違い。
> *   **カーネル法（カーネルトリック）**: 非線形分離可能なデータを分類するための重要な技術。

### 2.6 K近傍法 (K-Nearest Neighbors, KNN)

**K近傍法**は、主に**分類**に使われるシンプルで直感的な手法です。未知のデータが与えられたときに、そのデータに「最も近い」K個の訓練データ（近傍点）を参照し、その多数決によってクラスを分類します。

#### 2.6.1 K近傍法の仕組み

1.  **距離の計算**: 未知のデータと、訓練データセット内の全てのデータとの距離を計算します。
2.  **K個の近傍点の選択**: 計算した距離が最も短いK個の訓練データを選択します。
3.  **多数決による分類**: 選択されたK個の近傍点が属するクラスの中で、最も多いクラスを未知のデータのクラスとして予測します。

**例**: K=5の場合、未知のデータの周りにある5つのデータ点を見て、その中で最も多いクラスに分類します。もしクラスAが1つ、クラスBが3つ、クラスCが1つなら、クラスBに分類されます。

#### 2.6.2 K近傍法の注意点

K近傍法はシンプルですが、いくつかの注意点があります。

*   **クラスごとのデータ量の偏り**: 特定のクラスのデータ量が極端に多い場合、未知のデータがそのクラスに分類されやすくなり、予測精度が低下する可能性があります。
*   **特徴量のスケール**: 各特徴量のスケール（値の範囲）が異なっていると、距離の計算に偏りが生じ、予測精度が悪化します。事前に**データの正規化や標準化**を行い、スケールを揃える必要があります。
*   **Kの値の選択**:
    *   **Kが小さすぎる場合（例: K=1）**: 外れ値の影響を受けやすく、ノイズに敏感になります。
    *   **Kが大きすぎる場合**: 遠く離れたデータの影響も受けるようになり、分類の境界が曖昧になり、予測精度が低下する可能性があります。適切なKの値は、交差検定などを用いて決定します。
*   **計算コスト**: 未知のデータが与えられるたびに、全ての訓練データとの距離を計算する必要があるため、データ量が多くなると計算コストが非常に高くなります。
*   **次元の呪い**: データの次元（特徴量の数）が大きくなると、データ間の距離の差が小さくなり、全てのデータが互いに「遠く」見えてしまう現象です。これにより、K近

G検定の学習に取り組む皆さん、こんにちは！
この章では、G検定で頻出する機械学習の様々な手法、特に「教師なし学習」「半教師あり学習」「強化学習」について、そしてモデルの評価方法について深く掘り下げていきます。一つ一つの概念を丁寧に理解し、G検定合格を目指しましょう！

---

## 階層的クラスタリング手法

**クラスタリング**とは、データの中から似た性質を持つものを集めてグループ（クラスタ）に分ける教師なし学習の手法です。ここでは、階層的にクラスタを形成していく手法について解説します。

### ウォード法（Ward's Method）

**ウォード法**は、**階層的クラスタリング**の一種で、クラスタを結合する際に、結合によって生じる**情報の損失（誤差平方和の増加）が最も小さい組み合わせ**を選択します。

簡単に言うと、「似たもの同士をくっつけたときに、そのグループ全体のまとまり（分散）が最も崩れないようにする」方法です。クラスタ内のデータがどれだけ密接に集まっているかを重視し、結合後のクラスタ内のばらつきが最小になるように結合を進めます。

> 📌 **G検定頻出ポイント**:
> *   ウォード法は、クラスタ内の**分散（ばらつき）を最小化**するようにクラスタを結合します。
> *   クラスタの結合による**誤差平方和の増加が最小**になるペアを選択します。

### 群平均

G検定対策の学習書へようこそ！
この章では、機械学習モデルがどれだけ「良い」モデルなのかを評価するための重要な指標と、モデルが陥りやすい「過学習」という問題、そしてそれを防ぐためのテクニックについて詳しく解説します。

---

## 第X章 モデルの評価と改善

### 1. 分類モデルの評価指標

分類モデルは、入力されたデータがどのカテゴリ（クラス）に属するかを予測するモデルです。例えば、「この画像は犬か猫か」「このメールはスパムか否か」といった判断を行います。この分類モデルの性能を評価するために、いくつかの重要な指標が使われます。

#### 1.1 混同行列（Confusion Matrix）

分類モデルの評価の基本となるのが **混同行列** です。これは、モデルの予測結果と実際の正解を比較し、その組み合わせをまとめた表です。

例えば、「犬」の画像を「犬」と判断するモデルを考えてみましょう。

1.  **真陽性（True Positive: TP）**: 正解が「犬」の画像を、モデルが正しく「犬」と判断したケース。
    *   例：「犬」の画像を「犬」と予測。
2.  **偽陰性（False Negative: FN）**: 正解が「犬」の画像を、モデルが誤って「犬以外」（例：猫）と判断したケース。
    *   例：「犬」の画像を「犬以外」と予測。
3.  **偽陽性（False Positive: FP）**: 正解が「犬以外」の画像を、モデルが誤って「犬」と判断したケース。
    *   例：「犬以外」の画像を「犬」と予測。
4.  **真陰性（True Negative: TN）**: 正解が「犬以外」の画像を、モデルが正しく「犬以外」と判断したケース。
    *   例：「犬以外」の画像を「犬以外」と予測。

これらの4つの要素は、以下の表のように整理できます。

| | **モデルが「犬」と判断** | **モデルが「犬以外」と判断** |
| :----------------------- | :--------------------------- | :------------------------------- |
| **正解が「犬」** | 真陽性（TP） | 偽陰性（FN） |
| **正解が「犬以外」** | 偽陽性（FP） | 真陰性（TN） |

> 📌 **G検定頻出ポイント**:
> 混同行列の各要素（TP, FN, FP, TN）の定義と、それぞれが何を意味するのかを正確に理解しておくことが重要です。特に、偽陽性（FP）と偽陰性（FN）は混同しやすいため、具体例でしっかり区別できるようにしましょう。

#### 1.2 代表的な評価指標

混同行列の要素を使って、分類モデルの性能を測るための代表的な指標がいくつかあります。

1.  **正解率（Accuracy）**
    *   **定義**: 全データのうち、モデルが正しく予測できた割合。最も一般的で直感的な指標です。
    *   **計算式**: `正解率 = (TP + TN) ÷ (TP + FP + FN + TN)`
        *   つまり、「正しく陽性と判断した数」と「正しく陰性と判断した数」の合計を、全データ数で割ったものです。
    *   **特徴**: クラスに属するデータの数のバランスが取れている場合に有効です。しかし、特定のクラスのデータが極端に少ない（不均衡データ）場合、正解率だけではモデルの性能を正しく評価できないことがあります。
        *   **例**: 99%が「正常」、1%が「異常」というデータで、全てのデータを「正常」と予測するモデルでも、正解率は99%になってしまいます。この場合、異常を見つける能力は全くないのに、正解率だけは高く見えてしまいます。

2.  **適合率（Precision）**
    *   **定義**: モデルが「陽性」と判断したデータのうち、実際に陽性だった割合。モデルの「陽性判断の正確性」を評価する指標です。
    *   **計算式**: `適合率 = TP ÷ (TP + FP)`
        *   つまり、「正しく陽性と判断した数」を、「モデルが陽性と判断した全ての数（正しく陽性と判断したもの＋誤って陽性と判断したもの）」で割ったものです。
    *   **特徴**: **偽陽性（FP）** を減らしたい場合に重視されます。
        *   **例**: スパムメールの分類で「スパム」と判断した場合、それが本当にスパムである確率が高いことが求められます。もし誤って通常のメールをスパムと判断（偽陽性）してしまうと、重要なメールを見落とすことになりかねません。このようなケースでは、適合率を重視します。

3.  **再現率（Recall）**
    *   **定義**: 実際の陽性データの中で、モデルが正しく陽性と判断できた割合。モデルの「陽性判断の網羅性」を評価する指標です。
    *   **計算式**: `再現率 = TP ÷ (TP + FN)`
        *   つまり、「正しく陽性と判断した数」を、「実際の陽性データ数（正しく陽性と判断したもの＋誤って陰性と判断したもの）」で割ったものです。
    *   **特徴**: **偽陰性（FN）** を減らしたい場合に重視されます。
        *   **例**: 病気の診断で「陽性（病気である）」と判断する場合、実際に病気である人を見逃さない（偽陰性を減らす）ことが非常に重要です。たとえ誤って健康な人を病気と診断（偽陽性）してしまっても、再検査で判明すれば大きな問題にはなりにくいです。このようなケースでは、再現率を重視します。

4.  **F値（F-measure / F1-score）**
    *   **定義**: 適合率と再現率のバランスを評価する指標。両者の**調和平均**で計算されます。
    *   **計算式**: `F値 = (2 × 適合率 × 再現率) ÷ (適合率 + 再現率)`
    *   **特徴**: 適合率と再現率は**トレードオフ**の関係にあることが多く、一方を上げるともう一方が下がる傾向があります。F値は、この両方をバランス良く評価したいときに使用されます。

> 📌 **G検定頻出ポイント**:
> 適合率と再現率のトレードオフの関係はG検定でよく問われます。どちらの指標を重視すべきかは、そのモデルが解決しようとしている課題によって異なります。具体例（病気診断、スパムメール）と合わせて理解しましょう。

**具体例で計算してみよう！**

先ほどの犬の分類モデルの混同行列に、具体的な数値を当てはめてみましょう。

| | **モデルが「犬」と判断** | **モデルが「犬以外」と判断** |
| :----------------------- | :--------------------------- | :------------------------------- |
| **正解が「犬」** | 450 (TP) | 50 (FN) |
| **正解が「犬以外」** | 50 (FP) | 100 (TN) |
| **合計** | 500 | 150 |

このデータを使って、各指標を計算します。全データ数は `450 + 50 + 50 + 100 = 650` です。

*   **正解率**: `(TP + TN) ÷ 全データ数 = (450 + 100) ÷ 650 = 550 ÷ 650 ≈ 0.85 (85%)`
*   **適合率**: `TP ÷ (TP + FP) = 450 ÷ (450 + 50) = 450 ÷ 500 = 0.9 (90%)`
*   **再現率**: `TP ÷ (TP + FN) = 450 ÷ (450 + 50) = 450 ÷ 500 = 0.9 (90%)`
*   **F値**: `(2 × 適合率 × 再現率) ÷ (適合率 + 再現率) = (2 × 0.9 × 0.9) ÷ (0.9 + 0.9) = 1.62 ÷ 1.8 = 0.9 (90%)`

このように、具体的な数値で計算することで、各指標の意味をより深く理解できます。

### 2. 回帰モデルの評価指標

回帰モデルは、連続的な数値（例：家の価格、気温、売上など）を予測するモデルです。分類モデルとは異なり、予測値と実際の値との「誤差」の大きさを評価します。

代表的な回帰モデルの評価指標は以下の通りです。

1.  **MSE（Mean Squared Error：平均二乗誤差）**
    *   **定義**: 実際の値と予測値の誤差を二乗し、その平均を取ったものです。
    *   **特徴**: 誤差を二乗するため、大きな誤差（**外れ値**）に対してペナルティが大きくなります。つまり、外れ値の影響を大きく受けやすい指標です。単位が元のデータの二乗になるため、直感的な解釈が難しい場合があります。

2.  **RMSE（Root Mean Squared Error：二乗平均平方根誤差）**
    *   **定義**: MSEの平方根を取ったものです。
    *   **計算式**: `RMSE = √MSE`
    *   **特徴**: MSEと同様に外れ値の影響を大きく受けますが、平方根を取ることで単位が元のデータと同じになり、直感的に誤差の大きさを理解しやすくなります。大きな誤差を許容したくない場合に用いられることが多いです。

3.  **MAE（Mean Absolute Error：平均絶対誤差）**
    *   **定義**: 実際の値と予測値の誤差の絶対値を取り、その平均を取ったものです。
    *   **特徴**: 誤差を絶対値で扱うため、MSEやRMSEに比べて外れ値の影響を受けにくいという特徴があります。誤差の大きさを素直に評価したい場合に適しています。

4.  **決定係数（Coefficient of Determination, R²）**
    *   **定義**: モデルがどれだけ実際の値を説明できているかを示す指標です。**寄与率**とも呼ばれます。
    *   **計算式**: 相関係数（R）を二乗することで求められます。
    *   **特徴**: 0から1の範囲を取り、1に近いほどモデルの予測が実際の値によく適合していることを示します。モデルの当てはまりの良さを評価するのに使われます。

> 📌 **G検定頻出ポイント**:
> MSE、RMSE、MAEの違い、特に外れ値に対する感度の違いは重要です。RMSEはMAEに比べて外れ値の影響を大きく受けることを覚えておきましょう。決定係数も回帰モデルの評価指標として頻出です。

### 3. 過学習（オーバーフィッティング）

機械学習モデルを訓練する上で、最も注意すべき問題の一つが**過学習**です。

#### 3.1 過学習とは

**過学習（Overfitting）** とは、モデルが訓練データに対して過度に適応しすぎてしまい、訓練データ以外の**未知のデータ（汎化データ）** に対する予測精度が悪くなってしまう現象のことです。別名、**過剰適合（かじょうてきごう）** とも呼ばれます。

例えるなら、テスト勉強で過去問を丸暗記してしまい、少しでも形式が変わると全く解けなくなる状態に似ています。過去問（訓練データ）には完璧に対応できても、本番のテスト（未知のデータ）では点数が取れない、という状況です。

モデルが複雑になるほど、過学習に陥りやすくなります。

*   **回帰の場合の過学習**:
    *   例えば、身長と体重の関係を予測するモデルを考えます。訓練データ（身長と体重の点）に対して、複雑な曲線で完璧にフィットさせようとすると、データ点の間や外側では不自然な予測をしてしまうことがあります。
    *   訓練データ上では誤差が非常に小さいですが、新しい人の身長から体重を予測しようとすると、かえって大きな誤差が出てしまうことがあります。

*   **分類の場合の過学習**:
    *   ある2つのクラス（例えば、犬と猫）を分ける境界線を学習する際、訓練データの個々の点に合わせすぎて、複雑でギザギザな境界線を描いてしまうことがあります。
    *   この境界線は訓練データ上の分類は完璧ですが、新しいデータが来たときに、少し位置がずれただけで誤った分類をしてしまう可能性が高まります。

#### 3.2 過学習の主な原因

過学習が発生する主な原因は以下の通りです。

*   **モデルが複雑すぎる**: モデルが持つパラメータの数が多すぎたり、表現力が豊かすぎたりすると、訓練データのノイズまで学習してしまい、汎化性能が低下します。
*   **訓練データに対して特徴量が多すぎる**: 訓練データの量に対して、モデルが利用する特徴量（説明変数）が多すぎると、モデルはデータの本質的なパターンではなく、個々のデータの特徴に過剰に反応してしまいます。
*   **訓練データの量が少ない**: 訓練データが少ないと、モデルは限られたデータからしか学習できないため、そのデータに過剰に適合しやすくなります。

#### 3.3 過学習の見極め方

過学習を見極めるには、**訓練誤差**と**汎化誤差**を比較することが重要です。

*   **訓練誤差**: モデルが訓練データに対してどれだけ正確に予測できているかを示す誤差。
*   **汎化誤差（テスト誤差）**: モデルが未知のデータ（テストデータ）に対してどれだけ正確に予測できているかを示す誤差。

通常、学習が進むにつれて訓練誤差と汎化誤差はともに減少していきます。しかし、ある時点を過ぎると、訓練誤差はさらに減少し続けるのに、汎化誤差は逆に増加し始めることがあります。この状態が**過学習**です。

![過学習の見極め方](https://via.placeholder.com/600x200?text=学習進捗と誤差の関係図)
*（図のイメージ：横軸に学習進捗、縦軸に誤差。訓練誤差は単調減少、汎化誤差は最初は減少し、ある点で最小になり、その後増加する。）*

#### 3.4 汎化誤差の要素

汎化誤差は、以下の3つの要素に分解して考えることができます。

1.  **バイアス（Bias）**:
    *   **定義**: モデルが単純すぎるために発生する誤差。データの本質的なパターンを捉えきれていない状態です。
    *   **特徴**: モデルの表現力が低く、訓練データに対しても予測精度が低い（**未学習**）状態になりやすいです。
    *   **例**: 身長と体重の関係を直線で予測しようとしたが、実際は曲線的な関係だった場合、直線モデルではデータの本質を捉えきれず、常に一定の誤差（バイアス）が生じます。

2.  **バリアンス（Variance）**:
    *   **定義**: モデルが複雑すぎるために発生する誤差。訓練データのわずかな変動やノイズに過剰に反応し、未知のデータに対して不安定な予測をしてしまう状態です。
    *   **特徴**: 訓練データに対しては非常に高い精度を示しますが、未知のデータに対する予測精度が低くなります（**過学習**）。
    *   **例**: 身長と体重の関係を非常に複雑な曲線で予測しようとした場合、訓練データの個々の点に完璧にフィットしすぎて、新しいデータが来たときに予測が大きくブレてしまうことがあります。

3.  **ノイズ（Noise）**:
    *   **定義**: データ自体に含まれる本質的な不確実性や測定誤差など、モデルではどうすることもできない誤差。
    *   **特徴**: どのようなモデルを使っても、この誤差を完全に減らすことはできません。

**バイアス-バリアンスのトレードオフ**

バイアスとバリアンスは**トレードオフ**の関係にあります。

*   モデルを単純化すると、バイアスは大きくなりますが、バリアンスは小さくなります。
*   モデルを複雑化すると、バイアスは小さくなりますが、バリアンスは大きくなります。

目標は、バイアスとバリアンスの合計（そしてノイズ）である汎化誤差が最小になるように、モデルの複雑さを調整することです。過学習は、バイアスが小さくバリアンスが大きい状態であると言えます。

> 📌 **G検定頻出ポイント**:
> 過学習の概念、訓練誤差と汎化誤差の関係、そしてバイアス-バリアンスのトレードオフはG検定で非常に重要なテーマです。それぞれの意味と関係性をしっかり理解しましょう。

### 4. 過学習を防ぐテクニック

過学習はモデルの汎化性能を低下させるため、これを防ぐための様々なテクニックが開発されています。

#### 4.1 スパース化

**スパース化**とは、モデルに使用する**特徴量（説明変数）の数を減らす手法**です。特徴量が多すぎると、モデルは訓練データに対して過剰に適合しやすくなるため、不要な特徴量を削減することで過学習を防ぎます。

*   **例**: 家の価格を予測する際に、築年数や広さなどの重要な特徴量だけでなく、壁の色や庭の石の数など、価格にほとんど影響しない特徴量まで使ってしまうと、モデルはそれらの些細な情報にまで過剰に反応してしまいます。スパース化は、このような不要な特徴量を自動的に選択し、モデルを単純化する効果があります。

#### 4.2 データ拡張（Data Augmentation）

**データ拡張**とは、既存の訓練データから、様々な加工（回転、拡大縮小、反転、色の調整など）を行うことで、**データのバリエーションを人工的に増やす手法**です。**データの水増し**とも呼ばれます。

*   **例**: 画像認識モデルで犬の画像を学習させる際、元の犬の画像を少し回転させたり、明るさを変えたり、左右反転させたりした画像を新たに訓練データに加えます。
*   これにより、モデルは「少し傾いた犬」「暗い場所の犬」など、多様な状況の犬を学習できるようになり、特定のデータに過剰に適応してしまうことを防ぎ、汎化性能が向上します。

#### 4.3 正則化（Regularization）

**正則化**とは、過学習を防ぐために、モデルの**パラメータ（重み）に制限をかける手法**です。学習の際に、誤差関数（損失関数）に**正則化項（ペナルティ項）** を加えることで、モデルが複雑になりすぎるのを抑制します。

*   **基本的な考え方**:
    *   モデルの学習は、通常、誤差関数（予測と正解のズレ）を最小化することを目指します。
    *   正則化では、この誤差関数に「モデルの複雑さ」を表す項（正則化項）を追加します。
    *   `最適化する値 = 誤差関数 + 正則化項`
    *   これにより、モデルは誤差を最小化しようとするだけでなく、同時にモデルの複雑さも最小化しようとします。結果として、過度に複雑なモデルになることを防ぎ、シンプルなモデルを学習するよう促します。
*   **制約付き最適化問題**: このように制約（正則化項）を設けて最適化する問題を「制約付き最適化問題」と呼びます。

正則化にはいくつかの種類があります。

1.  **L0正則化**:
    *   **定義**: 0ではない重みの数（非ゼロのパラメータの数）を正則化項として使用します。
    *   **特徴**: モデルに使用する特徴量を直接的に減らす効果がありますが、計算コストが高いというデメリットがあります。

2.  **L1正則化（Lasso回帰）**:
    *   **定義**: 重みの**絶対値の総和**を正則化項として使用します。
    *   **特徴**: 重要度の低い重みを強制的に0にする効果があります。これにより、不要な特徴量がモデルから取り除かれ、**特徴選択**が行われます。モデルがより解釈しやすくなるという利点もあります。
    *   **ラッソ回帰**: L1正則化を適用した線形回帰モデルです。

3.  **L2正則化（Ridge回帰）**:
    *   **定義**: 重みの**二乗和**を正則化項として使用します。
    *   **特徴**: 重みが大きくなりすぎないように、重みの値を全体的に小さくする効果があります。これにより、モデルが滑らかになり、特定のデータに過剰に反応するのを防ぎます。
    *   **リッジ回帰**: L2正則化を適用した線形回帰モデルです。

4.  **Elastic Net**:
    *   **定義**: L1正則化とL2正則化を組み合わせたものです。
    *   **特徴**: L1正則化による特徴選択と、L2正則化による重みの平滑化の両方の効果を期待できます。

> 📌 **G検定頻出ポイント**:
> 正則化の概念、L1正則化とL2正則化の違い（特にL1が特徴選択の効果を持つ点）、そしてそれぞれが適用される回帰モデル（ラッソ回帰、リッジ回帰）はG検定で頻出です。

**正則化の注意点**:
正則化を強くかけすぎると、モデルが単純になりすぎてしまい、訓練データに対してさえ予測精度が低い**未学習（Underfitting）** の状態に陥る危険性があります。適切な強さで正則化を行うことが重要です。

**モデルの表現力と汎化性能**:
**モデルの表現力**とは、訓練データに対してどの程度まで複雑なパターンを説明できるかという能力です。表現力が高いモデルは、訓練データに完璧にフィットできますが、過学習のリスクが高まります。
モデルの表現力と汎化性能もまた**トレードオフ**の関係にあります。

#### 4.4 ノルムと正則化

正則化のL1やL2は、数学的な概念である**ノルム**に基づいています。

**ノルム**とは、ベクトルなどの「大きさ」を測るための尺度です。

1.  **L1ノルム（マンハッタン距離）**:
    *   **定義**: ベクトルの各要素の**絶対値の総和**によって求められる距離です。
    *   **例**: 2次元ベクトル `z = (4, 3)` のL1ノルムは `|4| + |3| = 7` です。
    *   L1正則化は、このL1ノルムの考え方を重みに適用したものです。

2.  **L2ノルム（ユークリッド距離）**:
    *   **定義**: ベクトルの各要素の**二乗和の平方根**によって求められる距離です。私たちが日常的に使う直線距離と同じです。
    *   **例**: 2次元ベクトル `z = (4, 3)` のL2ノルムは `√(4² + 3²) = √(16 + 9) = √25 = 5` です。
    *   L2正則化は、このL2ノルムの考え方を重みに適用したものです。

### 5. ROC曲線とモデルの選択

分類モデルの評価には、適合率や再現率だけでなく、**ROC曲線**というグラフもよく用いられます。

#### 5.1 ROC曲線とAUC

**ROC曲線（Receiver Operating Characteristic curve）** は、分類モデルの性能を視覚的に評価するためのグラフです。

*   **縦軸**: **真陽性率（TPR: True Positive Rate）**
    *   実際の陽性データのうち、モデルが正しく陽性と判断できた割合。再現率と同じです。
    *   `TPR = TP ÷ (TP + FN)`
*   **横軸**: **偽陽性率（FPR: False Positive Rate）**
    *   実際の陰性データのうち、モデルが誤って陽性と判断してしまった割合。
    *   `FPR = FP ÷ (FP + TN)`

ROC曲線は、分類のしきい値（閾値）を変化させたときの真陽性率と偽陽性率の関係を描いたものです。

![ROC曲線](https://via.placeholder.com/400x300?text=ROC曲線)
*（図のイメージ：横軸FPR、縦軸TPR。左下から右上に向かって曲線が伸びる。対角線はランダムな予測。）*

*   **理想的なモデル**: 左上隅（FPR=0, TPR=1）に近づくほど、偽陽性を出さずに真陽性を多く捉えられている、つまり性能が高いモデルと言えます。
*   **ランダムな予測**: 対角線（FPR=TPRの直線）は、ランダムに予測するモデルの性能を示します。

**AUC（Area Under the Curve）** は、ROC曲線の下の面積のことです。

*   **特徴**: AUCの値は0から1の範囲を取り、1に近いほどモデルの性能が高いと判断されます。
*   **解釈**: AUCが0.5のとき、それはランダムな予測を行うモデルと同程度の性能と判断できます。AUCが0.5を下回る場合は、モデルがランダムな予測よりも悪い性能を示していることになります。

> 📌 **G検定頻出ポイント**:
> ROC曲線とAUCの定義、特に真陽性率と偽陽性率がそれぞれ何を表すのか、そしてAUCの値がモデル性能をどう示すのかはG検定で頻出です。

#### 5.2 モデルの選択と情報量基準

タスクに対して最適なモデルを選択することは非常に重要です。

*   **複雑なモデル**: 複雑なタスクを実行できたり、予測精度を上げたりする可能性がありますが、一般的に学習コストが高く、過学習に陥りやすいという特徴があります。
*   **簡単なモデル**: 学習コストが低く、解釈しやすいですが、表現力が低く、未学習になるリスクがあります。

もし、複数のモデルが同程度の予測精度を示すのであれば、**簡単なモデルを選択するべき**です。これは「ある事柄を説明するためには、必要以上に多くを仮定するべきではない」という**オッカムの剃刀（Occam's Razor）** の考え方に基づいています。簡単なモデルの方が、汎化性能が高く、安定している傾向があるためです。

**情報量基準**は、モデルの複雑さと、データへの適合度とのバランスを見るために使用される指標です。

1.  **AIC（赤池情報量規準：Akaike Information Criterion）**
    *   **定義**: モデルの複雑さと、データへの当てはまりの良さ（尤度）を考慮して、モデルの良さを評価する指標です。
    *   **特徴**: 値が小さいほど良いモデルとされます。データ数が増えても、モデルの複雑さに対するペナルティは比較的緩やかです。

2.  **BIC（ベイズ情報量規準：Bayesian Information Criterion）**
    *   **定義**: AICと同様にモデルの良さを評価する指標ですが、AICよりもモデルの複雑さに対するペナルティが大きいです。
    *   **特徴**: 値が小さいほど良いモデルとされます。計算にデータ数が大きく関与するため、データ数が大きい場合には、AICよりもシンプルなモデルが評価されやすい傾向があります。

> 📌 **G検定頻出ポイント**:
> オッカムの剃刀の原則、そしてAICとBICという代表的な情報量基準の概念と、データ数による評価の違い（特にBICがデータ数に大きく影響される点）はG検定で問われることがあります。

---

## 📝 章末まとめ

この章では、機械学習モデルの評価と改善に関する重要な概念を学びました。G検定で特に問われやすいポイントを以下にまとめます。

*   **分類モデルの評価**:
    *   **混同行列**: TP（真陽性）、FN（偽陰性）、FP（偽陽性）、TN（真陰性）の4つの要素を理解する。
    *   **正解率**: 全体的な正しさ。不均衡データには注意が必要。
    *   **適合率**: 陽性判断の正確性（FPを減らしたい場合に重視）。
    *   **再現率**: 陽性判断の網羅性（FNを減らしたい場合に重視）。
    *   **F値**: 適合率と再現率のバランス評価（調和平均）。
    *   **適合率と再現率はトレードオフの関係**にある。

*   **回帰モデルの評価**:
    *   **MSE**: 誤差の二乗平均。外れ値に敏感。
    *   **RMSE**: MSEの平方根。単位が元のデータと同じで解釈しやすい。外れ値に敏感。
    *   **MAE**: 誤差の絶対値平均。外れ値に強い。
    *   **決定係数（R²）**: モデルの当てはまりの良さ（0～1）。

*   **過学習（オーバーフィッティング）**:
    *   訓練データに過度に適応し、未知データへの予測精度が低下する現象。
    *   **原因**: モデルが複雑すぎる、訓練データに対して特徴量が多すぎる、訓練データが少ない。
    *   **見極め方**: 訓練誤差は減少するが、汎化誤差が増加する。
    *   **汎化誤差の要素**: **バイアス**（単純すぎ）、**バリアンス**（複雑すぎ）、ノイズ。
    *   **バイアスとバリアンスはトレードオフの関係**にある。

*   **過学習を防ぐテクニック**:
    *   **スパース化**: 不要な特徴量を減らす。
    *   **データ拡張

