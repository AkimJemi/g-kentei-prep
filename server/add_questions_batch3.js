import pkg from 'pg';
const { Pool } = pkg;
const pool = new Pool({ connectionString:'postgresql://g_kentei_prep_app_db_user:0vZFHekJvsuMexPcBCKx5Ix4Noy7WZJO@dpg-d63nv6cr85hc73bckig0-a.oregon-postgres.render.com/g_kentei_prep_app_db', ssl:{rejectUnauthorized:false} });

const DL_ELEM = 'ディープラーニングの要素技術';
const DL_APP = 'ディープラーニングの応用例';
const MATH = 'AIに必要な数理・統計知識';

const questions = [
  // ===== ディープラーニングの要素技術 =====
  { cat:DL_ELEM, q:'「バッチ正規化（Batch Normalization）」の主な効果として正しいものはどれか。', opts:['バッチサイズを1に固定すること','各層の入力を正規化することで学習を安定・高速化し、過学習を抑える','バッチ処理の順番をランダムにすること','活性化関数を不要にすること'], ans:1, exp:'バッチ正規化はミニバッチ内の各特徴量の平均を0・標準偏差を1に正規化する手法です。内部共変量シフトを抑え、高い学習率での学習安定化・収束高速化・正則化効果（ドロップアウトと組み合わせ不要になることも）をもたらします。', optExp:['バッチサイズを1にするのはオンライン学習（SGD）です。','正解。各層入力の正規化で学習安定化・高速化・正則化効果があります。','バッチ順序のランダム化はシャッフルであり別の話です。','活性化関数は依然として必要です。'] },
  { cat:DL_ELEM, q:'Transformerアーキテクチャの「Self-Attention（自己注意機構）」の説明として正しいものはどれか。', opts:['入力シーケンスの各要素が他のすべての要素との関連性を計算することで文脈を考慮した表現を学ぶ機構','画像のピクセルを自動的にフォーカスする機構','RNNの隠れ状態を自動的に更新する機構','データを自動的に正規化する機構'], ans:0, exp:'Self-Attentionは入力シーケンス（例：文章の単語系列）の各トークンが、同じシーケンス内の他のすべてのトークンとの重み付き関係を計算することで、距離に関係なく文脈を把握できる機構です。Transformerの核心技術です。', optExp:['正解。各要素が他全要素との関連性を計算して文脈を学ぶ機構です。','Transformerは画像処理にも応用されますが、自己注意の定義は各要素間の関連性計算です。','RNNとは別のアーキテクチャです。','正規化とは異なります。'] },
  { cat:DL_ELEM, q:'残差接続（Skip Connection / Residual Connection）がResNetで導入された主な理由はどれか。', opts:['モデルのパラメータ数を大幅に削減するため','深いネットワークでの勾配消失問題を緩和し、非常に深いモデルの学習を可能にするため','計算速度を2倍にするため','過学習を完全に防止するため'], ans:1, exp:'ResNetのSkip Connectionは、入力を数層先に直接加算することで、勾配が浅い層まで伝わりやすくなる（勾配消失軽減）仕組みです。これにより100層を超えるような超深層ネットワークの学習が可能になりました。', optExp:['パラメータ数削減が主目的ではありません。','正解。勾配消失問題を緩和して超深層ネットワークの学習を可能にします。','速度2倍化が主目的ではありません。','過学習完全防止は目的ではありません。'] },
  { cat:DL_ELEM, q:'「双方向RNN（Bidirectional RNN）」の特徴として正しいものはどれか。', opts:['2台のコンピュータで分散学習すること','入力シーケンスを順方向と逆方向の両方向から処理し、各時刻点で過去と未来の両方の文脈を捉えること','2種類の異なるデータセットで学習すること','2つの損失関数を同時に使用すること'], ans:1, exp:'双方向RNNは順方向（過去→現在）と逆方向（未来→現在）の2つのRNNを組み合わせることで、ある時刻点でその前後両方の文脈情報を活用できます。感情分析・固有表現認識などNLPタスクで有効です。', optExp:['分散学習とは異なります。','正解。双方向処理で各時刻点での過去・未来両方の文脈を捉えます。','データセット数の話ではありません。','損失関数の数の話ではありません。'] },
  { cat:DL_ELEM, q:'「Max Pooling（最大プーリング）」の説明として正しいものはどれか。', opts:['領域内の画素値の平均を計算する操作','領域内の最大値を抽出する操作で、特徴の位置に対するロバスト性と次元削減をもたらす','最も大きな畳み込み層を選択する操作','レイヤーの最大数を設定すること'], ans:1, exp:'Max Poolingは特徴マップの局所領域から最大値を抽出する操作です。最も強い特徴シグナルを保持しながら空間解像度を削減（次元圧縮）し、小さな位置のずれ（局所的な平行移動）への不変性もある程度持ちます。', optExp:['平均を計算するのはAverage Poolingです。','正解。領域内の最大値を抽出し特徴の位置不変性と次元削減をもたらします。','畳み込み層の選択ではありません。','レイヤー数の設定ではありません。'] },
  { cat:DL_ELEM, q:'「オートエンコーダ（Autoencoder）」の基本的な構造として正しいものはどれか。', opts:['入力を高次元に拡張するエンコーダのみで構成される','入力と同じ次元を出力するエンコーダとデコーダで構成され、入力の圧縮表現（潜在変数）を学習する','3つ以上の異なるネットワークを並列に接続した構造','入力を分類する最終層のみで構成される'], ans:1, exp:'オートエンコーダはエンコーダ（入力→圧縮表現へ次元削減）とデコーダ（圧縮表現→元の次元に復元）から構成され、入力を再構成する（入力=出力目標）ように学習します。次元削減・異常検知・生成モデルの基礎として活用されます。', optExp:['エンコーダのみではありません。エンコーダとデコーダのペアです。','正解。エンコーダとデコーダで入力の圧縮表現を学習する構造です。','3つ以上の並列ネットワーク構造ではありません。','分類層のみではありません。'] },
  { cat:DL_ELEM, q:'「勾配消失問題」が深層ネットワークで発生する主な原因はどれか。', opts:['ネットワークのパラメータ数が少なすぎること','誤差逆伝播法によって勾配が深い層から浅い層に伝わる際に、活性化関数の微分値が繰り返し掛け合わされ勾配が指数的に小さくなること','GPUメモリが不足すること','学習データが多すぎること'], ans:1, exp:'Sigmoid等の活性化関数は微分値が最大0.25程度と小さく、逆伝播で何十層にも繰り返し掛け合わされると勾配が極めて小さくなります（消失）。解決策としてReLU活性化関数、ResNetのSkip Connection、バッチ正規化等が有効です。', optExp:['パラメータ数の少なさは学習不足の原因ですが、勾配消失とは別です。','正解。逆伝播での微分値の繰り返し乗算による指数的減衰が原因です。','GPUメモリ不足は計算資源の問題で、勾配消失とは別です。','データ量は勾配消失の直接原因ではありません。'] },
  { cat:DL_ELEM, q:'「データ拡張（Data Augmentation）」の主な目的はどれか。', opts:['テストデータを水増しして精度評価を有利にすること','学習データに変換（反転・回転・切り抜き等）を加えることでデータの多様性を増やし、モデルの汎化性能を向上させること','モデルのパラメータを自動的に増やすこと','クラウドストレージのデータを増やすこと'], ans:1, exp:'データ拡張は画像の反転・回転・色調変化・切り抜き等の変換を加えることで、実質的な学習データ量を増やし、様々なバリエーションへの対応力（汎化性能）を上げる手法です。特にデータが少ない場合に有効です。', optExp:['テストデータへの適用は不正な評価につながります。','正解。変換で多様性を増やし汎化性能を向上させることが目的です。','モデルパラメータは自動増加しません。','クラウドストレージとは無関係です。'] },
  // ===== ディープラーニングの応用例 =====
  { cat:DL_APP, q:'「YOLO（You Only Look Once）」はどのようなAIタスクに使われるか。', opts:['自然言語翻訳','リアルタイム物体検出（一回のNN通過で複数の物体の位置とクラスを同時に推定）','音声認識','テキスト生成'], ans:1, exp:'YOLOは画像全体を一度だけニューラルネットワークに通す（You Only Look Once）ことで、複数の物体のバウンディングボックスとクラスラベルをリアルタイムで高速に検出するアーキテクチャです。', optExp:['翻訳はSeq2Seqや Transformerの応用です。','正解。YOLOはリアルタイム物体検出モデルです。','音声認識はWhisper等のモデルが代表例です。','テキスト生成はGPT等のLLMが代表例です。'] },
  { cat:DL_APP, q:'「BERT（Bidirectional Encoder Representations from Transformers）」の特徴として正しいものはどれか。', opts:['画像生成に特化したモデル','Transformerの双方向エンコーダを使い、マスク言語モデリング等の事前学習で文脈を捉えた言語表現を学習する','一方向のテキスト生成に特化したモデル','音声認識専用のモデル'], ans:1, exp:'BERTはGoogleが2018年に発表したTransformer双方向エンコーダモデルです。マスク単語予測・次文予測で大規模コーパスを事前学習し、質問応答・固有表現認識・感情分析等の下流タスクをファインチューニングで高精度化できます。', optExp:['BERTは言語モデルであり画像生成に特化したものではありません。','正解。双方向Transformerで文脈を捉えた言語表現を学習します。','BERTは双方向であり、一方向生成に特化したGPTとは対照的です。','音声認識専用ではありません。'] },
  { cat:DL_APP, q:'生成AIにおける「RAG（Retrieval-Augmented Generation）」の説明として正しいものはどれか。', opts:['ランダムにデータを生成する手法','LLMの生成時に外部知識ベースから関連情報を検索・取得して回答の精度・最新性を向上させる手法','画像を自動でラベリングする手法','モデルの重みを定期的にリセットする手法'], ans:1, exp:'RAGはLLM（大規模言語モデル）が回答生成する際に、社内文書・最新情報等の外部データベースから関連文書を検索（Retrieve）し、それを文脈に加えて回答（Generate）する手法です。LLMの知識の限界・ハルシネーションを補う実用的アプローチです。', optExp:['ランダム生成とは異なります。','正解。外部知識を検索して組み込むことでLLMの精度・最新性を向上させます。','自動ラベリングはアノテーションの話です。','重みのリセットはRAGとは無関係です。'] },
  { cat:DL_APP, q:'「敵対的生成ネットワーク（GAN）」の学習プロセスの説明として正しいものはどれか。', opts:['1つのネットワークが教師データと自分の出力を比較して学習する','Generator（生成器）とDiscriminator（識別器）の2つのネットワークが互いに競い合うことで高品質なデータを生成できるようになる','強化学習のエージェントと環境が競い合う仕組み','複数のAIが多数決で答えを決める仕組み'], ans:1, exp:'GANはGeneratorが偽データを生成し、Discriminatorが本物か偽物かを判別するという敵対的な学習を繰り返します。Generatorは識別器を騙せるほどリアルなデータを生成しようとし、Discriminatorはより精度良く見分けようとします。この競合で生成品質が向上します。', optExp:['1つのネットワークのみの学習はオートエンコーダ等の話です。','正解。Generator とDiscriminatorの競合学習でリアルなデータ生成を実現します。','GANは強化学習の枠組みではありません。','多数決はアンサンブルの話です。'] },
  { cat:DL_APP, q:'「ファインチューニング（Fine-tuning）」の説明として正しいものはどれか。', opts:['モデルの精度を細かく測定すること','大規模データで事前学習したモデルのパラメータを、対象タスクの少量データで追加学習して特化させること','AIモデルを小型デバイス向けに圧縮すること','ハイパーパラメータを自動調整すること'], ans:1, exp:'ファインチューニングは事前学習済みモデル（GPTやBERT、ResNet等）の重みを初期値として、目的タスクのデータで追加学習する手法です。少ないデータ・少ない計算でも高精度なモデルを構築できます。', optExp:['精度測定はモデル評価の話です。','正解。事前学習モデルを対象タスクで追加学習して特化させる手法です。','モデル圧縮は量子化・プルーニング等の話です。','ハイパーパラメータ自動調整はAutoMLやベイズ最適化等の話です。'] },
  // ===== 数理・統計 =====
  { cat:MATH, q:'「ベイズの定理」の説明として正しいものはどれか。', opts:['大きな標本では標本平均が母平均に近づくという定理','事前確率と尤度から事後確率を計算する定理','二項分布が正規分布に近似できることを示す定理','行列の固有値を求めるための定理'], ans:1, exp:'ベイズの定理はP(A|B) = P(B|A)×P(A) / P(B) という式で表され、事前確率P(A)にデータBの尤度P(B|A)を組み合わせて事後確率P(A|B)を更新します。スパムフィルタ・医療診断・機械学習の確率的モデルで広く活用されます。', optExp:['大数の法則の説明です。','正解。事前確率と尤度から事後確率を計算するのがベイズの定理です。','中心極限定理の説明です。','固有値分解はベイズ定理とは別の数学的手法です。'] },
  { cat:MATH, q:'「マクローリン展開」や「テイラー展開」が機械学習の文脈で関係するのはどれか。', opts:['データの可視化','勾配降下法の理論的基礎として関数の局所的な近似に使われる','クラスタリングの距離計算','正規化の計算'], ans:1, exp:'テイラー展開は関数を多項式で局所近似する手法です。勾配降下法の更新則は一次テイラー近似に基づいており、ニュートン法は二次近似を使います。XGBoostなどの勾配ブースティングでも損失関数のテイラー展開が理論基盤となっています。', optExp:['可視化は別の話です。','正解。勾配降下法の理論的基礎として関数の局所近似に使われます。','距離計算はユークリッド距離等の話です。','正規化計算とは異なります。'] },
  { cat:MATH, q:'機械学習で使われる「コサイン類似度」の説明として正しいものはどれか。', opts:['2点間のユークリッド距離を測る指標','2つのベクトル間の角度のコサインで、方向の類似性（-1〜1）を測る指標','行列の行列式を計算する指標','データの標準偏差を計算する指標'], ans:1, exp:'コサイン類似度は2ベクトルのなす角のコサイン値（-1〜1）で、1に近いほど方向が似ており（高類似）、0で直交（無関係）、-1で逆方向です。大きさではなく方向の類似性を測るため、テキストの意味的類似度計算等に広く使われます。', optExp:['ユークリッド距離は2点間の直線距離で、コサイン類似度とは異なります。','正解。ベクトル間の角度コサインで方向の類似性(-1〜1)を測ります。','行列式は行列の性質を表すスカラー値です。','標準偏差はデータのばらつきを示します。'] },
  { cat:MATH, q:'「尤度（Likelihood）」の説明として正しいものはどれか。', opts:['実際に観測されたデータが、あるパラメータのモデルのもとで得られた確からしさ','テストデータでのモデルの正解率','ハイパーパラメータの設定値','データセットのサイズ'], ans:0, exp:'尤度L(θ|X)とは、パラメータθのモデルの下でデータXが観測される確率（確率密度）です。最尤推定（MLE）は「観測データを最もよく説明するパラメータθ」を求める方法で、機械学習の多くのアルゴリズムの理論基盤です。', optExp:['正解。観測データがあるパラメータのモデルで得られる確からしさが尤度です。','正解率はモデル評価の話です。','ハイパーパラメータは設計上の選択です。','データセットサイズは尤度の定義ではありません。'] },
  { cat:MATH, q:'「確率的勾配降下法（SGD）」の説明として正しいものはどれか。', opts:['全学習データを使って勾配を計算してパラメータを更新する手法','毎回1件（または小さなミニバッチ）のデータを使って勾配を近似的に計算し、パラメータを頻繁に更新する手法','確率的な乱数だけでパラメータを更新する手法','勾配を計算せずにパラメータをランダムに変更する手法'], ans:1, exp:'全データ（バッチ勾配降下法）の代わりに、毎回1件またはミニバッチのデータで勾配を近似計算しパラメータを更新します。計算コストが低く大規模データに対応でき、更新のノイズが局所最適解からの脱出に役立つこともあります。', optExp:['全データ使用はバッチ勾配降下法です。','正解。少量データで勾配を近似計算して頻繁に更新するのがSGDです。','純粋な乱数での更新ではありません。','勾配計算なしのランダム更新はSGDではありません。'] },
  { cat:MATH, q:'「行列の積（行列積）」の条件として正しいものはどれか。', opts:['2つの行列は必ず同じサイズでなければならない','行列A（m×n）と行列B（p×q）の積ABはn=pのとき定義され、結果はm×qの行列になる','行列積は常に可換（AB=BA）である','行列積の結果は常に正方行列である'], ans:1, exp:'行列積ABはAの「列数n」とBの「行数p」が等しい（n=p）ときのみ定義され、結果はAの行数（m）×Bの列数（q）の行列になります。ニューラルネットワークの層間の計算は行列積で表現されます。', optExp:['同じサイズでなくてもよく、内積が取れるサイズ関係が必要です。','正解。n=pのとき定義され結果はm×qの行列です。','行列積は一般に非可換です（AB≠BA）。','結果が正方行列とは限りません。'] },
  { cat:MATH, q:'「情報量（エントロピー）」の概念がAI・機械学習で使われる代表的な場面はどれか。', opts:['モデルの処理速度の測定','決定木の分岐点の選択基準として「どの特徴量で分割すれば最も情報利得が大きいか」を計算する','ニューラルネットワークのパラメータ数の計算','データセットのサイズの表現'], ans:1, exp:'シャノンのエントロピーH(X)=-Σp(x)log p(x)は不確実性（乱雑さ）を表します。決定木では分割前後のエントロピー差（情報利得）を最大にする特徴量で分割します。また、交差エントロピー損失関数もエントロピーの概念から派生しています。', optExp:['処理速度の測定にエントロピーは使いません。','正解。決定木の分割基準として情報利得（エントロピーの減少量）を計算します。','パラメータ数の計算にはエントロピーを使いません。','データサイズの表現とは異なります。'] },
];

async function main() {
  const client = await pool.connect();
  try {
    let count = 0;
    for (const q of questions) {
      await client.query(
        `INSERT INTO g_kentei_questions (category,question,options,correctAnswer,explanation,optionExplanations,source) VALUES ($1,$2,$3,$4,$5,$6,'system')`,
        [q.cat, q.q, JSON.stringify(q.opts), q.ans, q.exp, JSON.stringify(q.optExp)]
      );
      count++;
      process.stdout.write(`\r追加中: ${count}/${questions.length}`);
    }
    console.log(`\n✅ ${count}問追加完了`);
    const res = await client.query(`SELECT c.title, COUNT(q.id) as cnt FROM g_kentei_categories c LEFT JOIN g_kentei_questions q ON q.category=c.id GROUP BY c.title ORDER BY cnt::int ASC`);
    res.rows.forEach(r => console.log(`  ${r.title}: ${r.cnt}問`));
  } finally {
    client.release();
    await pool.end();
  }
}
main().catch(console.error);
