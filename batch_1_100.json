[
  {
    "id": 1,
    "question": "1956年に開催され、「人工知能」という言葉が初めて使われた会議は。",
    "options": "[\"ダートマス会議\",\"エニアック会議\",\"チューリング会議\",\"スタンフォード会議\"]",
    "explanation": "ジョン・マッカーシーらによって、人工知能の定義が話し合われました。"
  },
  {
    "id": 2,
    "question": "ジョン・サールが提唱した、人間のように意識や理解を持つAIを何と呼ぶか。",
    "options": "[\"強いAI\",\"弱いAI\",\"汎用AI\",\"特化型AI\"]",
    "explanation": "強いAIは、単なる計算ではなく、心や意識を持つとされます。"
  },
  {
    "id": 3,
    "question": "特定のタスク（チェスや画像認識など）のみを解決するAIの分類は。",
    "options": "[\"汎用AI\",\"特化型AI (Narrow AI)\",\"強いAI\",\"自律型AI\"]",
    "explanation": "現在のAIのほとんどは、特定のタスクに特化しています。"
  },
  {
    "id": 4,
    "question": "記号が現実世界の意味と結びつかない問題を何と呼ぶか。",
    "options": "[\"フレーム問題\",\"シンボルグラウンディング問題\",\"停止問題\",\"トロッコ問題\"]",
    "explanation": "AIが記号の意味を身体的経験なく理解することの難しさを示す問題です。"
  },
  {
    "id": 5,
    "question": "ロボットが限られた能力で、現実に起こりうる全事象を考慮できない問題を何というか。",
    "options": "[\"次元の呪い\",\"フレーム問題\",\"シンボル崩壊\",\"組み合わせの爆発\"]",
    "explanation": "有限の処理能力では、無限の可能性を計算しきれないという問題です。"
  },
  {
    "id": 6,
    "question": "知的な活動はAIに簡単だが、日常的な運動・感覚は難しいという逆説。",
    "options": "[\"モラベックのパラドックス\",\"チューリング・テスト\",\"中国の部屋\",\"フェルミのパラドックス\"]",
    "explanation": "高度な推論よりも、進化の歴史が長い感覚・運動機能の方が実装が困難です。"
  },
  {
    "id": 7,
    "question": "機械が知性を持っているかを判定するための「模倣ゲーム」を提案したのは。",
    "options": "[\"ジョン・マッカーシー\",\"アラン・チューリング\",\"マービン・ミンスキー\",\"ハーバート・サイモン\"]",
    "explanation": "1950年に「チューリング・テスト」として提案されました。"
  },
  {
    "id": 8,
    "question": "「中国語を理解していなくても、マニュアル通りに記号を操作すれば対話できる」という思考実験。",
    "options": "[\"エニアックの部屋\",\"チューリングの部屋\",\"中国の部屋\",\"マッカーシーの部屋\"]",
    "explanation": "ジョン・サールが「強いAI」に反論するために提唱しました。"
  },
  {
    "id": 9,
    "question": "ダートマス会議の主要メンバーで、認知心理学の父とも呼ばれる人物は。",
    "options": "[\"ハーバート・サイモン\",\"レイ・カーツワイル\",\"アンドリュー・ン\",\"ヨシュア・ベンジオ\"]",
    "explanation": "Logic Theoristを開発し、AIと心理学の架け橋となりました。"
  },
  {
    "id": 10,
    "question": "第一次AIブームにおける、迷路やチェスなどの「おもちゃ」のような問題の呼称。",
    "options": "[\"トイ・プロブレム\",\"ショート・プロブレム\",\"シンプル・ケース\",\"ダミー・パズル\"]",
    "explanation": "現実の複雑な問題は解けず、整理されたパズルしか解けないことを揶揄した言葉です。"
  },
  {
    "id": 11,
    "question": "アラン・チューリングが「計算する機械と知能」を執筆した年は。",
    "options": "[\"1946年\",\"1950年\",\"1956年\",\"1960年\"]",
    "explanation": "この論文でチューリング・テストが提案されました。"
  },
  {
    "id": 12,
    "question": "人工知能を「知的な機械を構築するための科学と技術」と定義したのは。",
    "options": "[\"ジョン・マッカーシー\",\"ジェフリー・ヒントン\",\"サム・アルトマン\",\"イーロン・マスク\"]",
    "explanation": "1955年の提案書などで述べられています。"
  },
  {
    "id": 13,
    "question": "記号を物理的に操作することで思考を実現するという仮説。",
    "options": "[\"身体性仮説\",\"物理記号システム仮説\",\"コネクショニズム\",\"量子脳理論\"]",
    "explanation": "ニューウェルとサイモンが提唱した、記号主義AIの根幹です。"
  },
  {
    "id": 14,
    "question": "第一次AIブームで「推論」を担当した、初期の証明プログラムの名前は。",
    "options": "[\"Logic Theorist\",\"General Problem Solver\",\"ELIZA\",\"MYCIN\"]",
    "explanation": "ニューウェル、ショー、サイモンによって開発されました。"
  },
  {
    "id": 15,
    "question": "「弱いAI」が人間を超える知能を持つようになっても意識はないとする立場は。",
    "options": "[\"機能主義\",\"行動主義\",\"消去主義\",\"独我論\"]",
    "explanation": "外部から見て知的な行動をしていれば、中身の意識は問わないとする考えです。"
  },
  {
    "id": 16,
    "question": "身体が知覚や環境との相互作用に不可欠であるとする考え方。",
    "options": "[\"身体性 (Embodiment)\",\"脱身体性\",\"二元論\",\"唯物論\"]",
    "explanation": "シンボルグラウンディング問題を解決する鍵とも考えられています。"
  },
  {
    "id": 17,
    "question": "AI研究における「合理的なエージェント」という定義での成功指標は。",
    "options": "[\"人間の模倣\",\"思考の正確性\",\"環境における報酬の最大化\",\"計算速度\"]",
    "explanation": "目標達成のために最適な行動をとることに重点を置きます。"
  },
  {
    "id": 18,
    "question": "初期の自然言語処理プログラムで、来談者中心療法のセラピストを模したのは。",
    "options": "[\"ALICE\",\"SIRI\",\"ELIZA\",\"SHRDLU\"]",
    "explanation": "ジョセフ・ワイゼンバウムによって1966年に開発されました。"
  },
  {
    "id": 19,
    "question": "積み木の世界のような限定的な環境を扱うAIプログラム。",
    "options": "[\"SHRDLU\",\"Deep Blue\",\"AlphaGo\",\"Watson\"]",
    "explanation": "テリー・ウィノグラードが開発し、記号処理の成功例とされました。"
  },
  {
    "id": 20,
    "question": "記号主義に対抗し、ニューラルネットワークなどの結合に基づく考え方。",
    "options": "[\"コネクショニズム\",\"フォルマリズム\",\"構造主義\",\"記号主義\"]",
    "explanation": "脳の構造を模したアプローチを指します。"
  },
  {
    "id": 21,
    "question": "第2次AIブームの中心となった技術。",
    "options": "[\"ディープラーニング\",\"エキスパートシステム\",\"ビッグデータ\",\"強化学習\"]",
    "explanation": "専門家の知識を「if-then」のルールで記述しました。"
  },
  {
    "id": 22,
    "question": "2012年、ディープラーニングが注目されるきっかけとなったコンペ。",
    "options": "[\"Kaggle\",\"ILSVRC\",\"RoboCup\",\"DARPA\"]",
    "explanation": "画像認識の精度が劇的に向上しました。"
  },
  {
    "id": 23,
    "question": "2045年に、AIが人類の知覚を超えるとする予測。",
    "options": "[\"ビッグバン\",\"シンギュラリティ\",\"デジタルトランスフォーメーション\",\"メタバース\"]",
    "explanation": "レイ・カーツワイルが提唱しました。"
  },
  {
    "id": 24,
    "question": "2011年、クイズ番組「ジェパディ！」で人間に勝利したIBMのAI。",
    "options": "[\"Deep Blue\",\"Watson\",\"Blue Gene\",\"HAL\"]",
    "explanation": "非構造化データからの質問応答に優れていました。"
  },
  {
    "id": 25,
    "question": "AIが特定のタスクで高い能力を示す一方で、一般的配慮が欠けることの揶揄。",
    "options": "[\"バカなAI\",\"サヴァン症候群のようなAI\",\"ゴースト・イン・ザ・シェル\",\"ブラックボックス\"]",
    "explanation": "限定的な能力に特化していることを指します。"
  },
  {
    "id": 26,
    "question": "日本のAI戦略2019で掲げられた、AI人材の目標数（年間）。",
    "options": "[\"10万人\",\"25万人\",\"50万人\",\"100万人\"]",
    "explanation": "リテラシー教育を含めた野心的な目標です。"
  },
  {
    "id": 27,
    "question": "データが指数関数的に増大している現状を示すキーワード。",
    "options": "[\"オープンデータ\",\"ビッグデータ\",\"IoT\",\"ブロックチェーン\"]",
    "explanation": "第3次AIブームを支えるインフラです。"
  },
  {
    "id": 28,
    "question": "AIブームの合間に訪れた、研究費削減や関心低下の時期。",
    "options": "[\"AIの秋\",\"AIの冬\",\"AIの氷河期\",\"AIの停滞期\"]",
    "explanation": "過度な期待の反動で訪れました。"
  },
  {
    "id": 29,
    "question": "AlphaGoがイ・セドルに勝利した対局が行われた都市は。",
    "options": "[\"東京\",\"北京\",\"ソウル\",\"サンフランシスコ\"]",
    "explanation": "2016年に全世界が注目しました。"
  },
  {
    "id": 30,
    "question": "大規模言語モデル (LLM) の急激な進歩をもたらした2017年の論文。",
    "options": "[\"Attention is All You Need\",\"ResNet\",\"Deep Residual Learning\",\"ImageNet Classification\"]",
    "explanation": "Transformerが提案されました。"
  },
  {
    "id": 31,
    "question": "OpenAIが2022年11月に公開し、世界的旋風を巻き起こした AI。",
    "options": "[\"GPT-2\",\"GPT-3\",\"ChatGPT\",\"GPT-4\"]",
    "explanation": "対話型AIの普及を決定づけました。"
  },
  {
    "id": 32,
    "question": "AI研究を民主化することを目的として設立された非営利団体。",
    "options": "[\"Google Brain\",\"DeepMind\",\"OpenAI\",\"Meta AI\"]",
    "explanation": "当初は非営利としてスタートしました。"
  },
  {
    "id": 33,
    "question": "2024年のAIトレンドとして注目されている、端末上でAIを動かす技術。",
    "options": "[\"クラウドAI\",\"エッジAI\",\"ハイブリッドAI\",\"ローカルLLM\"]",
    "explanation": "プライバシー保護や低遅延のために重要です。"
  },
  {
    "id": 34,
    "question": "第2次AIブームを支えた、日本の国家プロジェクト。",
    "options": "[\"第五世代コンピュータ\",\"AI-Japan\",\"スーパーコンピュータ京\",\"ムーンショット計画\"]",
    "explanation": "並列推論マシンの開発を目指しました。"
  },
  {
    "id": 35,
    "question": "収穫加速の法則 (The Law of Accelerating Returns) を提唱したのは。",
    "options": "[\"レイ・カーツワイル\",\"ビル・ジョイ\",\"ニック・ボストロム\",\"ユヴァル・ノア・ハラリ\"]",
    "explanation": "進化が指数関数的に加速するという考えです。"
  },
  {
    "id": 36,
    "question": "2010年代に起きた、ニューラルネットワークの多層化によるブレイクスルー。",
    "options": "[\"機械学習\",\"統計的学習\",\"ディープラーニング\",\"データマイニング\"]",
    "explanation": "ジェフリー・ヒントンらによる再発見です。"
  },
  {
    "id": 37,
    "question": "AIが自分自身でより高度なAIを作るようになり、知能が爆発する現象。",
    "options": "[\"再帰的自己改善\",\"自己複製\",\"オートメーション\",\"特異点\"]",
    "explanation": "シンギュラリティのメカニズムです。"
  },
  {
    "id": 38,
    "question": "2024年成立、EUが制定した包括的なAI規制法。",
    "options": "[\"GDPR\",\"EU AI Act\",\"DMA\",\"DSA\"]",
    "explanation": "世界的なAIガバナンスの基準となります。"
  },
  {
    "id": 39,
    "question": "AI技術が、テキスト・画像・音声を統合して扱えるようになること。",
    "options": "[\"ユニバーサルAI\",\"マルチモーダル\",\"シングルモーダル\",\"クロスプラットフォーム\"]",
    "explanation": "GPT-4Vなどがその代表です。"
  },
  {
    "id": 40,
    "question": "画像生成AI「Stable Diffusion」などがソースコードを公開している形態。",
    "options": "[\"クローズドソース\",\"プロプライエタリ\",\"オープンソース\",\"フリーウェア\"]",
    "explanation": "コミュニティ主導の発展を促しました。"
  },
  {
    "id": 41,
    "question": "データの「ラベル」を予測する問題。",
    "options": "[\"分類\",\"回帰\",\"クラスタリング\",\"次元削減\"]",
    "explanation": "犬か猫か、などの種類を当てます。"
  },
  {
    "id": 42,
    "question": "数値を予測する問題（住宅価格の予測など）。",
    "options": "[\"分類\",\"回帰\",\"クラスタリング\",\"次元削減\"]",
    "explanation": "連続値を求めます。"
  },
  {
    "id": 43,
    "question": "モデルが訓練データに合いすぎて、未知データに弱くなること。",
    "options": "[\"学習不足\",\"過学習\",\"汎化不全\",\"バイアス\"]",
    "explanation": "ノイズまで学習してしまいます。"
  },
  {
    "id": 44,
    "question": "データをk個に分割し、順番にテスト用として使い精度を測る。",
    "options": "[\"ホールドアウト法\",\"交差検証 (Cross Validation)\",\"ブートストラップ\",\"ランダムサンプリング\"]",
    "explanation": "データの偏りを防ぎます。"
  },
  {
    "id": 45,
    "question": "多数の決定木を使い、多数決や平均で予測する。",
    "options": "[\"ランダムフォレスト\",\"ロジスティック回帰\",\"k近傍法\",\"SVM\"]",
    "explanation": "バギングの代表例です。"
  },
  {
    "id": 46,
    "question": "距離が近いデータk個のラベルから、自分のラベルを決める。",
    "options": "[\"k-means\",\"k近傍法\",\"k次元木\",\"k主成分\"]",
    "explanation": "シンプルな非線形分類です。"
  },
  {
    "id": 47,
    "question": "損失関数にパルミ（罰則）を加え、重みの肥大化を抑える。",
    "options": "[\"正則化\",\"非線形化\",\"正規化\",\"標準化\"]",
    "explanation": "L1正則化、L2正則化などがあります。"
  },
  {
    "id": 48,
    "question": "エージェントが「報酬」を最大化するように行動を学ぶ。",
    "options": "[\"強化学習\",\"教師あり学習\",\"教師なし学習\",\"自己学習\"]",
    "explanation": "試行錯誤を繰り返します。"
  },
  {
    "id": 49,
    "question": "データを2つのグループに分ける際、境界とデータの距離を最大化する。",
    "options": "[\"サポートベクターマシン (SVM)\",\"PCA\",\"LDA\",\"決定木\"]",
    "explanation": "マージン最大化が特徴です。"
  },
  {
    "id": 50,
    "question": "高次元のデータを、情報を保ちつつ低次元に圧縮する。",
    "options": "[\"クラスタリング\",\"主成分分析 (PCA)\",\"回帰\",\"ブースティング\"]",
    "explanation": "教師なし学習の代表例です。"
  },
  {
    "id": 51,
    "question": "「真（ Positive ）」であるもののうち、正しく「真」と予測できた割合。",
    "options": "[\"精度 (Accuracy)\",\"再現率 (Recall)\",\"適合率 (Precision)\",\"F値\"]",
    "explanation": "見逃しを減らしたいときに重視します。"
  },
  {
    "id": 52,
    "question": "「真」と予測したもののうち、本当に「真」であった割合。",
    "options": "[\"精度\",\"再現率\",\"適合率\",\"特異度\"]",
    "explanation": "誤検知を減らしたいときに重視します。"
  },
  {
    "id": 53,
    "question": "「はい」か「いいえ」の2つのカテゴリに分けること。",
    "options": "[\"2クラス分類\",\"多クラス分類\",\"単回帰\",\"多変量解析\"]",
    "explanation": "バイナリ分類とも呼びます。"
  },
  {
    "id": 54,
    "question": "複数のモデルを並列に学習させ、結果を平均する手法。",
    "options": "[\"バギング\",\"ブースティング\",\"スタッキング\",\"正則化\"]",
    "explanation": "Bootstrap Aggregatingの略です。"
  },
  {
    "id": 55,
    "question": "誤ったデータの重みを増やし、逐次的にモデルを強化する。",
    "options": "[\"バギング\",\"ブースティング\",\"正規化\",\"プーリング\"]",
    "explanation": "AdaBoostやXGBoostがあります。"
  },
  {
    "id": 56,
    "question": "データを木構造で表現し、条件分岐で予測する。",
    "options": "[\"ニューラルネットワーク\",\"決定木\",\"ロジスティック回帰\",\"ナイーブベイズ\"]",
    "explanation": "解釈性が高いのがメリットです。"
  },
  {
    "id": 57,
    "question": "確率モデルの一つで、スパム判定によく使われる。",
    "options": "[\"ナイーブベイズ (単純ベイズ)\",\"LDA\",\"Q学習\",\"SARSA\"]",
    "explanation": "ベイズの定理を応用しています。"
  },
  {
    "id": 58,
    "question": "類似したデータを自動的にまとめる（ラベルなし）。",
    "options": "[\"回帰\",\"物体検出\",\"クラスタリング\",\"転移学習\"]",
    "explanation": "k-means法などがあります。"
  },
  {
    "id": 59,
    "question": "現在の状態 $s$ と行動 $a$ に基づき、将来の報酬を予測する関数。",
    "options": "[\"Q関数\",\"P関数\",\"Loss関数\",\"Sigmoid関数\"]",
    "explanation": "Q学習の基本概念です。"
  },
  {
    "id": 60,
    "question": "ハイパーパラメータを自動で探索し、最適なモデルを作る技術。",
    "options": "[\"AutoML\",\"DeepLearning\",\"CloudComputing\",\"DataMining\"]",
    "explanation": "モデル構築の自動化です。"
  },
  {
    "id": 61,
    "question": "入力に重みをかけ、合計してバイアスを足したものを何に通すか。",
    "options": "[\"損失関数\",\"活性化関数\",\"評価関数\",\"目的関数\"]",
    "explanation": "ニューロンの発火を制御します。"
  },
  {
    "id": 62,
    "question": "出力と正解の「ズレ」を計算する関数。",
    "options": "[\"活性化関数\",\"損失関数\",\"恒等関数\",\"基底関数\"]",
    "explanation": "この値を最小化するように学習します。"
  },
  {
    "id": 63,
    "question": "重みを更新する方向と量を決めるアルゴリズム。",
    "options": "[\"オプティマイザ (最適化手法)\",\"アクティベータ\",\"正規化器\",\"初期化器\"]",
    "explanation": "Adam, SGD, AdaGradなど。"
  },
  {
    "id": 64,
    "question": "勾配を逆向きに伝える、効率的な学習アルゴリズム。",
    "options": "[\"順伝播\",\"誤差逆伝播法\",\"ランダムウォーク\",\"モンテカルロ法\"]",
    "explanation": "バックプロパゲーション。"
  },
  {
    "id": 65,
    "question": "層が深くなると勾配が0に近づき、学習が進まなくなること。",
    "options": "[\"勾配爆発\",\"勾配消失\",\"過学習\",\"次元の呪い\"]",
    "explanation": "シグモイド関数などで発生しやすい。"
  },
  {
    "id": 66,
    "question": "勾配消失対策として普及した、負を0、正をそのまま出す関数。",
    "options": "[\"シグモイド\",\"tanh\",\"ReLU\",\"ソフトマックス\"]",
    "explanation": "現在の主流です。"
  },
  {
    "id": 67,
    "question": "一度の更新に使うデータのまとまり。",
    "options": "[\"全データ\",\"ミニバッチ\",\"シングルデータ\",\"ランダムデータ\"]",
    "explanation": "メモリ効率と安定性のバランスをとります。"
  },
  {
    "id": 68,
    "question": "全データを一度学習し終えることの単位。",
    "options": "[\"バッチ\",\"イテレーション\",\"エポック\",\"ステップ\"]",
    "explanation": "何エポック学習させるかを決めます。"
  },
  {
    "id": 69,
    "question": "入力層付近まで大きな勾配が伝わらず、重みが変化しなくなる問題。",
    "options": "[\"勾配消失\",\"局所解\",\"鞍点\",\"プラトー\"]",
    "explanation": "層を深くする際の最大の障壁でした。"
  },
  {
    "id": 70,
    "question": "中間層の出力を調整し、学習を安定・高速化させる手法。",
    "options": "[\"ドロップアウト\",\"バッチ正規化\",\"早期終了\",\"データの水増し\"]",
    "explanation": "Batch Normalization。"
  },
  {
    "id": 71,
    "question": "学習の初期に重みをどのように設定すべきか。",
    "options": "[\"すべて0にする\",\"すべて1にする\",\"適切な初期化手法で乱数にする\",\"前回の値を引き継ぐ\"]",
    "explanation": "Heの初期化、Xavierの初期化などがあります。"
  },
  {
    "id": 72,
    "question": "学習中に、ある一部のニューロンを無視して過学習を防ぐ。",
    "options": "[\"ドロップアウト\",\"プーリング\",\"畳み込み\",\"回帰\"]",
    "explanation": "モデルの表現力を適度に制限します。"
  },
  {
    "id": 73,
    "question": "ネットワークが学習するパラメータ（重みやバイアス）以外の調整値。",
    "options": "[\"ハイパーパラメータ\",\"内部パラメータ\",\"隠れパラメータ\",\"静的パラメータ\"]",
    "explanation": "学習率や層の数など、人間が設定します。"
  },
  {
    "id": 74,
    "question": "損失関数が最小に見えるが、実は全体ではもっと低い場所がある地点。",
    "options": "[\"局所最適解 (ローカルミニマム)\",\"大域的最適解\",\"鞍点\",\"平坦地\"]",
    "explanation": "ここから抜け出す必要があります。"
  },
  {
    "id": 75,
    "question": "微分係数が0で、ある方向からは極小だが別からは極大である点。",
    "options": "[\"ピーク\",\"鞍点 (サドルポイント)\",\"ボトム\",\"特異点\"]",
    "explanation": "高次元空間では局所解よりこちらが問題になります。"
  },
  {
    "id": 76,
    "question": "学習率を徐々に小さくしていくスケジュール。",
    "options": "[\"LRScheduler\",\"Optimizer\",\"LinearScaler\",\"StepFilter\"]",
    "explanation": "最後に細かく調整します。"
  },
  {
    "id": 77,
    "question": "モデルの誤差（損失）を最小化する方向を求める。",
    "options": "[\"勾配降下法\",\"線形計画法\",\"二分探索\",\"深さ優先探索\"]",
    "explanation": "Deep Learningの基本アルゴリズムです。"
  },
  {
    "id": 78,
    "question": "モデルを訓練するデータの他に、ハイパーパラメータ調整に使うデータ。",
    "options": "[\"テストデータ\",\"検証データ (Validation)\",\"本番データ\",\"公開データ\"]",
    "explanation": "学習状況を監視します。"
  },
  {
    "id": 79,
    "question": "勾配消失対策で、活性化関数の前に正規化を挿入すること。",
    "options": "[\"バッチ正規化\",\"層正規化 (Layer Norm)\",\"インスタンス正規化\",\"すべて正しい\"]",
    "explanation": "様々な正規化手法が存在します。"
  },
  {
    "id": 80,
    "question": "初期のニューラルネットワーク研究。",
    "options": "[\"パーセプトロン\",\"コネクショニスト\",\"サイバネティクス\",\"ニューロ\"]",
    "explanation": "ローゼンブラットが1958年に発表。"
  },
  {
    "id": 81,
    "question": "CNNにおける、フィルタを通して特徴を抽出する層。",
    "options": "[\"プーリング層\",\"全結合層\",\"畳み込み層\",\"反転層\"]",
    "explanation": "画像のパターンを捉えます。"
  },
  {
    "id": 82,
    "question": "CNNにおいて「ズレ」を許容し、データを小さくする層。",
    "options": "[\"プーリング層\",\"畳み込み層\",\"リカレント層\",\"正規化層\"]",
    "explanation": "最大値をとるMax Poolingが有名です。"
  },
  {
    "id": 83,
    "question": "文脈や時系列を扱う、再帰的な構造を持つ。",
    "options": "[\"CNN\",\"RNN\",\"GAN\",\"VAE\"]",
    "explanation": "Recurrent Neural Network。"
  },
  {
    "id": 84,
    "question": "RNNの勾配消失を「セル」と「ゲート」で解決した。",
    "options": "[\"LSTM\",\"GRU\",\"BERT\",\"ResNet\"]",
    "explanation": "Long Short-Term Memory。"
  },
  {
    "id": 85,
    "question": "層をバイパス（スキップ接続）し、超多層化を可能にした。",
    "options": "[\"VGGNet\",\"GoogLeNet\",\"ResNet\",\"AlexNet\"]",
    "explanation": "Residual Network。"
  },
  {
    "id": 86,
    "question": "Transformerの中核で、どこに注目するかを計算する。",
    "options": "[\"Convolution\",\"Pooling\",\"Attention (自己注目)\",\"Recursion\"]",
    "explanation": "Self-Attention機構。"
  },
  {
    "id": 87,
    "question": "2つのAIを競わせ、偽物と本物の判別を磨く。",
    "options": "[\"VAE\",\"GAN\",\"GAN-BERT\",\"StyleTransfer\"]",
    "explanation": "敵対的生成ネットワーク。"
  },
  {
    "id": 88,
    "question": "入力から特徴を抽出し、また元に戻すことで学習する。",
    "options": "[\"オートエンコーダ\",\"RNN\",\"CNN\",\"パーセプトロン\"]",
    "explanation": "次元削減等に使われます。"
  },
  {
    "id": 89,
    "question": "画像内のピクセルごとにラベルを付けるタスク。",
    "options": "[\"画像分類\",\"物体検出\",\"セマンティックセグメンテーション\",\"顔認証\"]",
    "explanation": "道路、歩行者、建物、などに塗り分けます。"
  },
  {
    "id": 90,
    "question": "物体がどこにあり、何であるかを四角（バウンディングボックス）で囲む。",
    "options": "[\"画像分類\",\"物体検出\",\"セグメンテーション\",\"生成\"]",
    "explanation": "YOLOなどが有名です。"
  },
  {
    "id": 91,
    "question": "CNNの畳み込みフィルタは何を学習するか。",
    "options": "[\"画素値そのもの\",\"重み（カーネルパラメータ）\",\"バイアスのみ\",\"データのラベル\"]",
    "explanation": "エッジや色などのパターンを学習します。"
  },
  {
    "id": 92,
    "question": "系列データから系列データを作る（例：翻訳）。",
    "options": "[\"Seq2Seq\",\"RNN\",\"MLP\",\"DQN\"]",
    "explanation": "Encoder-Decoder構造。"
  },
  {
    "id": 93,
    "question": "訓練済みモデルを別のタスクへ引き継ぐ。",
    "options": "[\"転移学習\",\"追加学習\",\"強化学習\",\"汎化\"]",
    "explanation": "少量のデータで高精度が得られます。"
  },
  {
    "id": 94,
    "question": "強化学習の行動の価値を、Deep Learningで近似する手法。",
    "options": "[\"DQN\",\"Q-Learning\",\"SARSA\",\"MonteCarlo\"]",
    "explanation": "Deep Q-Network。"
  },
  {
    "id": 95,
    "question": "CNNにおいて、フィルタをスライドさせる幅。",
    "options": "[\"パディング\",\"ストライド\",\"カーネルサイズ\",\"チャンネル\"]",
    "explanation": "出力サイズに影響します。"
  },
  {
    "id": 96,
    "question": "CNNで、入力の周りを0などで埋め、サイズを維持する。",
    "options": "[\"ストライド\",\"パディング\",\"プーリング\",\"ドロップアウト\"]",
    "explanation": "端の情報を捨てないようにします。"
  },
  {
    "id": 97,
    "question": "RNNの改良型で、LSTMよりパラメータを簡略化したもの。",
    "options": "[\"GRU\",\"BERT\",\"ELMo\",\"RoBERTa\"]",
    "explanation": "Gated Recurrent Unit。"
  },
  {
    "id": 98,
    "question": "GANが学習に失敗し、同じような画像しか出さなくなること。",
    "options": "[\"モード崩壊\",\"勾配消失\",\"バイアス\",\"バースト\"]",
    "explanation": "生成の多様性が失われます。"
  },
  {
    "id": 99,
    "question": "生成AIにおいて、データにノイズを加え、それを除去する過程を学ぶ。",
    "options": "[\"拡散モデル\",\"VAE\",\"GAN\",\"FlowModel\"]",
    "explanation": "Stable Diffusionなどの基礎です。"
  },
  {
    "id": 100,
    "question": "Googleが開発し、画像の分類性能で人間の精度を超えたモデル(2015)。",
    "options": "[\"ResNet\",\"Inception\",\"VGG\",\"MobileNet\"]",
    "explanation": "152層のネットワークです。"
  }
]